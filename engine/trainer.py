
# Ultralytics YOLO ğŸš€, AGPL-3.0 license
"""
Train a model on a dataset.

Usage:
    $ yolo mode=train model=yolov8n.pt data=coco8.yaml imgsz=640 epochs=100 batch=16
"""
# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¾ã™ã€‚
#
# ä½¿ç”¨æ³•ï¼š
#     $ yolo mode=train model=yolov8n.pt data=coco8.yaml imgsz=640 epochs=100 batch=16

import gc  # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’æä¾›ã—ã¾ã™ã€‚
import math  # æ•°å­¦é–¢æ•°ã‚’æä¾›ã—ã¾ã™ã€‚
import os  # ã•ã¾ã–ã¾ãªã‚ªãƒšãƒ¬ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ é–¢æ•°ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’æä¾›ã—ã¾ã™ã€‚
import subprocess  # ã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹ã‚’ä½œæˆã™ã‚‹ãŸã‚ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
import time  # æ™‚é–“é–¢é€£ã®é–¢æ•°ã‚’æä¾›ã—ã¾ã™ã€‚
import warnings  # è­¦å‘Šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å‡¦ç†ã‚’å¯èƒ½ã«ã—ã¾ã™ã€‚
from copy import copy, deepcopy  # ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ã‚³ãƒ”ãƒ¼æ“ä½œã‚’æä¾›ã—ã¾ã™ã€‚
from datetime import datetime, timedelta  # æ—¥ä»˜ã¨æ™‚é–“ã®æ“ä½œã‚’è¡Œã†ãŸã‚ã®ã‚¯ãƒ©ã‚¹ã‚’æä¾›ã—ã¾ã™ã€‚
from pathlib import Path  # ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ãƒ‘ã‚¹ã‚’æ“ä½œã™ã‚‹ãŸã‚ã®ã‚¯ãƒ©ã‚¹ã‚’æä¾›ã—ã¾ã™ã€‚

import numpy as np  # æ•°å€¤è¨ˆç®—ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã€‚
import torch  # PyTorchã®ãƒ¡ã‚¤ãƒ³ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã€‚
from torch import distributed as dist  # åˆ†æ•£ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹PyTorchãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã€‚
from torch import nn, optim  # ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’æä¾›ã—ã¾ã™ã€‚

from cfg import get_cfg, get_save_dir  # è¨­å®šé–¢é€£ã®é–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from data.utils import check_cls_dataset, check_det_dataset  # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒã‚§ãƒƒã‚¯é–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from nn.tasks import attempt_load_one_weight, attempt_load_weights  # ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰é–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from utils import (  # ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
    DEFAULT_CFG,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
    LOCAL_RANK,  # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ©ãƒ³ã‚¯
    LOGGER,  # ãƒ­ã‚¬ãƒ¼
    RANK,  # ãƒ©ãƒ³ã‚¯
    TQDM,  # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼
    callbacks,  # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°
    clean_url,  # URLã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã™ã‚‹é–¢æ•°
    colorstr,  # ã‚«ãƒ©ãƒ¼æ–‡å­—åˆ—
    emojis,  # çµµæ–‡å­—
    yaml_save,  # YAMLä¿å­˜é–¢æ•°
)
from utils.autobatch import check_train_batch_size  # è‡ªå‹•ãƒãƒƒãƒã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯é–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from utils.checks import check_amp, check_file, check_imgsz, check_model_file_from_stem, print_args  # ãƒã‚§ãƒƒã‚¯é–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from utils.dist import ddp_cleanup, generate_ddp_command  # åˆ†æ•£ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–¢é€£ã®é–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from utils.files import get_latest_run  # æœ€æ–°ã®å®Ÿè¡Œã‚’å–å¾—ã™ã‚‹é–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from utils.torch_utils import (  # PyTorchãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
    TORCH_2_4,  # PyTorchã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒ2.4ä»¥é™ã‹ã©ã†ã‹
    EarlyStopping,  # æ—©æœŸåœæ­¢ã‚¯ãƒ©ã‚¹
    ModelEMA,  # ãƒ¢ãƒ‡ãƒ«EMAã‚¯ãƒ©ã‚¹
    autocast,  # è‡ªå‹•æ··åˆç²¾åº¦
    convert_optimizer_state_dict_to_fp16,  # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã®çŠ¶æ…‹è¾æ›¸ã‚’fp16ã«å¤‰æ›
    init_seeds,  # ã‚·ãƒ¼ãƒ‰ã‚’åˆæœŸåŒ–
    one_cycle,  # OneCycleLRã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©
    select_device,  # ãƒ‡ãƒã‚¤ã‚¹ã‚’é¸æŠ
    strip_optimizer,  # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã‚’å‰Šé™¤
    torch_distributed_zero_first,  # åˆ†æ•£ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã®ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
    unset_deterministic,
    )


class BaseTrainer:
    # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚’ä½œæˆã™ã‚‹ãŸã‚ã®åŸºæœ¬ã‚¯ãƒ©ã‚¹ã€‚
    #
    # å±æ€§ï¼š
    #     args (SimpleNamespace): ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã®æ§‹æˆã€‚
    #     validator (BaseValidator): ãƒãƒªãƒ‡ãƒ¼ã‚¿ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã€‚
    #     model (nn.Module): ãƒ¢ãƒ‡ãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã€‚
    #     callbacks (defaultdict): ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã®è¾æ›¸ã€‚
    #     save_dir (Path): çµæœã‚’ä¿å­˜ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã€‚
    #     wdir (Path): é‡ã¿ã‚’ä¿å­˜ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã€‚
    #     last (Path): æœ€å¾Œã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¸ã®ãƒ‘ã‚¹ã€‚
    #     best (Path): æœ€é«˜ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¸ã®ãƒ‘ã‚¹ã€‚
    #     save_period (int): xã‚¨ãƒãƒƒã‚¯ã”ã¨ã«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜ã—ã¾ã™ï¼ˆ1æœªæº€ã®å ´åˆã¯ç„¡åŠ¹ï¼‰ã€‚
    #     batch_size (int): ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®ãƒãƒƒãƒã‚µã‚¤ã‚ºã€‚
    #     epochs (int): ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã‚¨ãƒãƒƒã‚¯æ•°ã€‚
    #     start_epoch (int): ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®é–‹å§‹ã‚¨ãƒãƒƒã‚¯ã€‚
    #     device (torch.device): ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒã‚¤ã‚¹ã€‚
    #     amp (bool): AMPï¼ˆè‡ªå‹•æ··åˆç²¾åº¦ï¼‰ã‚’æœ‰åŠ¹ã«ã™ã‚‹ãƒ•ãƒ©ã‚°ã€‚
    #     scaler (amp.GradScaler): AMPã®å‹¾é…ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã€‚
    #     data (str): ãƒ‡ãƒ¼ã‚¿ã¸ã®ãƒ‘ã‚¹ã€‚
    #     trainset (torch.utils.data.Dataset): ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‚
    #     testset (torch.utils.data.Dataset): ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‚
    #     ema (nn.Module): ãƒ¢ãƒ‡ãƒ«ã®EMAï¼ˆæŒ‡æ•°ç§»å‹•å¹³å‡ï¼‰ã€‚
    #     resume (bool): ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å†é–‹ã—ã¾ã™ã€‚
    #     lf (nn.Module): æå¤±é–¢æ•°ã€‚
    #     scheduler (torch.optim.lr_scheduler._LRScheduler): å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã€‚
    #     best_fitness (float): é”æˆã•ã‚ŒãŸæœ€é«˜ã®é©åˆåº¦ã€‚
    #     fitness (float): ç¾åœ¨ã®é©åˆåº¦ã€‚
    #     loss (float): ç¾åœ¨ã®æå¤±å€¤ã€‚
    #     tloss (float): åˆè¨ˆæå¤±å€¤ã€‚
    #     loss_names (list): æå¤±åã®ãƒªã‚¹ãƒˆã€‚
    #     csv (Path): çµæœCSVãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒ‘ã‚¹ã€‚

    def __init__(self, cfg=DEFAULT_CFG, overrides=None, _callbacks=None):
        # BaseTrainerã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–ã—ã¾ã™ã€‚
        #
        # å¼•æ•°ï¼š
        #     cfg (str, optional): æ§‹æˆãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒ‘ã‚¹ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯DEFAULT_CFGã§ã™ã€‚
        #     overrides (dict, optional): æ§‹æˆã®ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Noneã§ã™ã€‚
        self.args = get_cfg(cfg, overrides)  # è¨­å®šã‚’å–å¾—
        self.check_resume(overrides)  # ãƒ¬ã‚¸ãƒ¥ãƒ¼ãƒ ã‚’ãƒã‚§ãƒƒã‚¯
        self.device = select_device(self.args.device, self.args.batch)  # ãƒ‡ãƒã‚¤ã‚¹ã‚’é¸æŠ
        self.validator = None  # ãƒãƒªãƒ‡ãƒ¼ã‚¿ãƒ¼
        self.metrics = None  # ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        self.plots = {}  # ãƒ—ãƒ­ãƒƒãƒˆ
        init_seeds(self.args.seed + 1 + RANK, deterministic=self.args.deterministic)  # ã‚·ãƒ¼ãƒ‰ã‚’åˆæœŸåŒ–

        # Dirs
        self.save_dir = get_save_dir(self.args)  # ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å–å¾—
        self.args.name = self.save_dir.name  # update name for loggersã€‚ãƒ­ã‚¬ãƒ¼ã®åå‰ã‚’æ›´æ–°
        self.wdir = self.save_dir / "weights"  # weights dirã€‚é‡ã¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        if RANK in {-1, 0}:  # ãƒ©ãƒ³ã‚¯ãŒ-1ã¾ãŸã¯0ã®å ´åˆ
            self.wdir.mkdir(parents=True, exist_ok=True)  # make dirã€‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
            self.args.save_dir = str(self.save_dir)  # ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜å…ˆ
            yaml_save(self.save_dir / "args.yaml", vars(self.args))  # save run argsã€‚å®Ÿè¡Œå¼•æ•°ã‚’ä¿å­˜
        self.last, self.best = self.wdir / "last.pt", self.wdir / "best.pt"  # checkpoint pathsã€‚ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ‘ã‚¹
        self.save_period = self.args.save_period  # ä¿å­˜é–“éš”

        self.batch_size = self.args.batch  # ãƒãƒƒãƒã‚µã‚¤ã‚º
        self.epochs = self.args.epochs  # ã‚¨ãƒãƒƒã‚¯æ•°
        self.start_epoch = 0  # é–‹å§‹ã‚¨ãƒãƒƒã‚¯
        if RANK == -1:  # ãƒ©ãƒ³ã‚¯ãŒ-1ã®å ´åˆ
            print_args(vars(self.args))  # å¼•æ•°ã‚’å‡ºåŠ›

        # Device
        if self.device.type in {"cpu", "mps"}:  # ãƒ‡ãƒã‚¤ã‚¹ã‚¿ã‚¤ãƒ—ãŒcpuã¾ãŸã¯mpsã®å ´åˆ
            self.args.workers = 0  # faster CPU training as time dominated by inference, not dataloadingã€‚æ¨è«–ã«ã‚ˆã£ã¦æ™‚é–“ãŒæ”¯é…ã•ã‚Œã‚‹ãŸã‚ã€CPUãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒé«˜é€ŸåŒ–

        # Model and Dataset
        self.model = check_model_file_from_stem(self.args.model)  # add suffix, i.e. yolov8n -> yolov8n.ptã€‚ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’è¿½åŠ 
        with torch_distributed_zero_first(LOCAL_RANK):  # avoid auto-downloading dataset multiple timesã€‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã®ç¹°ã‚Šè¿”ã—ã‚’å›é¿
            self.trainset, self.testset = self.get_dataset()  # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å–å¾—
        self.ema = None  # EMAã‚’åˆæœŸåŒ–

        # Optimization utils init
        self.lf = None  # æå¤±é–¢æ•°ã‚’åˆæœŸåŒ–
        self.scheduler = None  # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’åˆæœŸåŒ–

        # Epoch level metrics
        self.best_fitness = None  # æœ€é«˜ã®é©åˆåº¦ã‚’åˆæœŸåŒ–
        self.fitness = None  # é©åˆåº¦ã‚’åˆæœŸåŒ–
        self.loss = None  # æå¤±ã‚’åˆæœŸåŒ–
        self.tloss = None  # åˆè¨ˆæå¤±ã‚’åˆæœŸåŒ–
        self.loss_names = ["Loss"]  # æå¤±åã‚’è¨­å®š
        self.csv = self.save_dir / "results.csv"  # csvãƒ‘ã‚¹ã‚’è¨­å®š
        self.plot_idx = [0, 1, 2]  # ãƒ—ãƒ­ãƒƒãƒˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹

        # HUB
        self.hub_session = None  # HUBã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’åˆæœŸåŒ–

        # Callbacks
        self.callbacks = _callbacks or callbacks.get_default_callbacks()  # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’å–å¾—
        if RANK in {-1, 0}:  # ãƒ©ãƒ³ã‚¯ãŒ-1ã¾ãŸã¯0ã®å ´åˆ
            callbacks.add_integration_callbacks(self)  # çµ±åˆã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’è¿½åŠ 

    def add_callback(self, event: str, callback):
        """Appends the given callback."""
        # æŒ‡å®šã•ã‚ŒãŸã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’è¿½åŠ ã—ã¾ã™ã€‚
        self.callbacks[event].append(callback)  # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’è¿½åŠ 

    def set_callback(self, event: str, callback):
        """Overrides the existing callbacks with the given callback."""
        # æŒ‡å®šã•ã‚ŒãŸã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§æ—¢å­˜ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ã—ã¾ã™ã€‚
        self.callbacks[event] = [callback]  # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’è¨­å®š

    def run_callbacks(self, event: str):
        """Run all existing callbacks associated with a particular event."""
        # ç‰¹å®šã®ã‚¤ãƒ™ãƒ³ãƒˆã«é–¢é€£ä»˜ã‘ã‚‰ã‚ŒãŸæ—¢å­˜ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ã™ã¹ã¦å®Ÿè¡Œã—ã¾ã™ã€‚
        for callback in self.callbacks.get(event, []):  # ã‚¤ãƒ™ãƒ³ãƒˆã«é–¢é€£ä»˜ã‘ã‚‰ã‚ŒãŸã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’åå¾©å‡¦ç†
            callback(self)  # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’å®Ÿè¡Œ

    def train(self):
        """Allow device='', device=None on Multi-GPU systems to default to device=0."""
        # è¤‡æ•°GPUã‚·ã‚¹ãƒ†ãƒ ã§device = ''ã€device = Noneã‚’ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§device = 0ã«ã—ã¾ã™ã€‚
        if isinstance(self.args.device, str) and len(self.args.device):  # i.e. device='0' or device='0,1,2,3'ã€‚deviceãŒæ–‡å­—åˆ—ã§ã€é•·ã•ãŒã‚ã‚‹å ´åˆ
            world_size = len(self.args.device.split(","))  # ãƒ‡ãƒã‚¤ã‚¹æ•°ã‚’è¨ˆç®—
        elif isinstance(self.args.device, (tuple, list)):  # i.e. device=[0, 1, 2, 3] (multi-GPU from CLI is list)ã€‚deviceãŒã‚¿ãƒ—ãƒ«ã¾ãŸã¯ãƒªã‚¹ãƒˆã®å ´åˆ
            world_size = len(self.args.device)  # ãƒ‡ãƒã‚¤ã‚¹æ•°ã‚’è¨ˆç®—
        elif self.args.device in {"cpu", "mps"}:  # i.e. device='cpu' or 'mps'ã€‚deviceãŒcpuã¾ãŸã¯mpsã®å ´åˆ
            world_size = 0  # ãƒ¯ãƒ¼ãƒ«ãƒ‰ã‚µã‚¤ã‚ºã‚’0ã«è¨­å®š
        elif torch.cuda.is_available():  # i.e. device=None or device='' or device=numberã€‚cudaãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆ
            world_size = 1  # default to device 0ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§device 0
        else:  # i.e. device=None or device=''ã€‚ãã‚Œä»¥å¤–ã®å ´åˆ
            world_size = 0  # ãƒ¯ãƒ¼ãƒ«ãƒ‰ã‚µã‚¤ã‚ºã‚’0ã«è¨­å®š

        # Run subprocess if DDP training, else train normally
        if world_size > 1 and "LOCAL_RANK" not in os.environ:  # ãƒ¯ãƒ¼ãƒ«ãƒ‰ã‚µã‚¤ã‚ºãŒ1ã‚ˆã‚Šå¤§ããã€LOCAL_RANKãŒç’°å¢ƒå¤‰æ•°ã«ãªã„å ´åˆ
            # Argument checks
            if self.args.rect:  # ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”å›ºå®šã®å ´åˆ
                LOGGER.warning("WARNING âš ï¸ 'rect=True' is incompatible with Multi-GPU training, setting 'rect=False'")  # è­¦å‘Šãƒ­ã‚°ã‚’å‡ºåŠ›
                self.args.rect = False  # ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”å›ºå®šã‚’ç„¡åŠ¹åŒ–
            if self.args.batch < 1.0:  # ãƒãƒƒãƒã‚µã‚¤ã‚ºãŒ1ã‚ˆã‚Šå°ã•ã„å ´åˆ
                LOGGER.warning(
                    "WARNING âš ï¸ 'batch<1' for AutoBatch is incompatible with Multi-GPU training, setting "
                    "default 'batch=16'"
                )  # è­¦å‘Šãƒ­ã‚°ã‚’å‡ºåŠ›
                self.args.batch = 16  # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’16ã«è¨­å®š

            # Command
            cmd, file = generate_ddp_command(world_size, self)  # DDPã‚³ãƒãƒ³ãƒ‰ã‚’ç”Ÿæˆ
            try:
                LOGGER.info(f'{colorstr("DDP:")} debug command {" ".join(cmd)}')  # DDPã‚³ãƒãƒ³ãƒ‰ã‚’å‡ºåŠ›
                subprocess.run(cmd, check=True)  # DDPã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œ
            except Exception as e:  # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆ
                raise e  # ã‚¨ãƒ©ãƒ¼ã‚’ç™ºç”Ÿ
            finally:  # æœ€å¾Œã«
                ddp_cleanup(self, str(file))  # DDPã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—

        else:  # é€šå¸¸ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
            self._do_train(world_size)  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œ

    def _setup_scheduler(self):
        """Initialize training learning rate scheduler."""
        # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’åˆæœŸåŒ–ã—ã¾ã™ã€‚
        if self.args.cos_lr:  # ã‚³ã‚µã‚¤ãƒ³å­¦ç¿’ç‡ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ
            self.lf = one_cycle(1, self.args.lrf, self.epochs)  # cosine 1->hyp['lrf']ã€‚ã‚³ã‚µã‚¤ãƒ³ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’åˆæœŸåŒ–
        else:  # ç·šå½¢å­¦ç¿’ç‡ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ
            self.lf = lambda x: max(1 - x / self.epochs, 0) * (1.0 - self.args.lrf) + self.args.lrf  # linearã€‚ç·šå½¢ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’åˆæœŸåŒ–
        self.scheduler = optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda=self.lf)  # ãƒ©ãƒ ãƒ€LRã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’åˆæœŸåŒ–

    def _setup_ddp(self, world_size):
        """Initializes and sets the DistributedDataParallel parameters for training."""
        # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã«DistributedDataParallelãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’åˆæœŸåŒ–ãŠã‚ˆã³è¨­å®šã—ã¾ã™ã€‚
        torch.cuda.set_device(RANK)  # ãƒ‡ãƒã‚¤ã‚¹ã‚’è¨­å®š
        self.device = torch.device("cuda", RANK)  # ãƒ‡ãƒã‚¤ã‚¹ã‚’è¨­å®š
        # LOGGER.info(f'DDP info: RANK {RANK}, WORLD_SIZE {world_size}, DEVICE {self.device}')
        os.environ["TORCH_NCCL_BLOCKING_WAIT"] = "1"  # set to enforce timeoutã€‚ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’å¼·åˆ¶ã™ã‚‹ãŸã‚ã«è¨­å®š
        dist.init_process_group(  # ãƒ—ãƒ­ã‚»ã‚¹ã‚°ãƒ«ãƒ¼ãƒ—ã‚’åˆæœŸåŒ–
            backend="nccl" if dist.is_nccl_available() else "gloo",  # ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚’è¨­å®š
            timeout=timedelta(seconds=10800),  # 3 hoursã€‚ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’è¨­å®š
            rank=RANK,  # ãƒ©ãƒ³ã‚¯ã‚’è¨­å®š
            world_size=world_size,  # ãƒ¯ãƒ¼ãƒ«ãƒ‰ã‚µã‚¤ã‚ºã‚’è¨­å®š
        )

    def _setup_train(self, world_size):
        """Builds dataloaders and optimizer on correct rank process."""
        # æ­£ã—ã„ãƒ©ãƒ³ã‚¯ãƒ—ãƒ­ã‚»ã‚¹ã§ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã¨ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚
        # Model
        self.run_callbacks("on_pretrain_routine_start")  # pretrainãƒ«ãƒ¼ãƒãƒ³ã®é–‹å§‹æ™‚ã«ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’å®Ÿè¡Œ
        ckpt = self.setup_model()  # ãƒ¢ãƒ‡ãƒ«ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
        self.model = self.model.to(self.device)  # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•
        self.set_model_attributes()  # ãƒ¢ãƒ‡ãƒ«å±æ€§ã‚’è¨­å®š

        # Freeze layers
        freeze_list = (  # ãƒ•ãƒªãƒ¼ã‚ºã™ã‚‹ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ãƒªã‚¹ãƒˆã‚’å–å¾—
            self.args.freeze
            if isinstance(self.args.freeze, list)
            else range(self.args.freeze)
            if isinstance(self.args.freeze, int)
            else []
        )
        always_freeze_names = [".dfl"]  # always freeze these layersã€‚å¸¸ã«ã“ã‚Œã‚‰ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’ãƒ•ãƒªãƒ¼ã‚º
        freeze_layer_names = [f"model.{x}." for x in freeze_list] + always_freeze_names  # ãƒ•ãƒªãƒ¼ã‚ºã™ã‚‹ãƒ¬ã‚¤ãƒ¤ãƒ¼å
        self.freeze_layer_names = freeze_layer_names
        for k, v in self.model.named_parameters():  # ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’åå¾©å‡¦ç†
            # v.register_hook(lambda x: torch.nan_to_num(x))  # NaN to 0 (commented for erratic training results)
            if any(x in k for x in freeze_layer_names):  # ãƒ¬ã‚¤ãƒ¤ãƒ¼åãŒãƒ•ãƒªãƒ¼ã‚ºãƒªã‚¹ãƒˆã«å«ã¾ã‚Œã¦ã„ã‚‹å ´åˆ
                LOGGER.info(f"Freezing layer '{k}'")  # ãƒ­ã‚°ã‚’å‡ºåŠ›
                v.requires_grad = False  # å‹¾é…è¨ˆç®—ã‚’ç„¡åŠ¹åŒ–
            elif not v.requires_grad and v.dtype.is_floating_point:  # only floating point Tensor can require gradientsã€‚æµ®å‹•å°æ•°ç‚¹ãƒ†ãƒ³ã‚½ãƒ«ã®ã¿ãŒå‹¾é…ã‚’å¿…è¦ã¨ã™ã‚‹
                LOGGER.info(
                    f"WARNING âš ï¸ setting 'requires_grad=True' for frozen layer '{k}'. "
                    "See ultralytics.engine.trainer for customization of frozen layers."
                )  # è­¦å‘Šãƒ­ã‚°ã‚’å‡ºåŠ›
                v.requires_grad = True  # å‹¾é…è¨ˆç®—ã‚’æœ‰åŠ¹åŒ–

        # Check AMP
        self.amp = torch.tensor(self.args.amp).to(self.device)  # True or Falseã€‚AMPã‚’æœ‰åŠ¹ã«ã™ã‚‹ã‹ã©ã†ã‹
        if self.amp and RANK in {-1, 0}:  # Single-GPU and DDPã€‚ã‚·ãƒ³ã‚°ãƒ«GPUã¨DDPã®å ´åˆ
            callbacks_backup = callbacks.default_callbacks.copy()  # backup callbacks as check_amp() resets themã€‚check_ampï¼ˆï¼‰ãŒãƒªã‚»ãƒƒãƒˆã™ã‚‹ãŸã‚ã€ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
            self.amp = torch.tensor(check_amp(self.model), device=self.device)  # check ampã€‚AMPã‚’ãƒã‚§ãƒƒã‚¯
            callbacks.default_callbacks = callbacks_backup  # restore callbacksã€‚ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’å¾©å…ƒ
        if RANK > -1 and world_size > 1:  # DDP
            dist.broadcast(self.amp, src=0)  # broadcast the tensor from rank 0 to all other ranks (returns None)ã€‚ãƒ©ãƒ³ã‚¯0ã‹ã‚‰ä»–ã®ã™ã¹ã¦ã®ãƒ©ãƒ³ã‚¯ã«ãƒ†ãƒ³ã‚½ãƒ«ã‚’ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆ
        self.amp = bool(self.amp)  # as booleanã€‚ãƒ–ãƒ¼ãƒ«å€¤ã¨ã—ã¦
        self.scaler = (
            torch.amp.GradScaler("cuda", enabled=self.amp) if TORCH_2_4 else torch.cuda.amp.GradScaler(enabled=self.amp)
        )  # å‹¾é…ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã‚’åˆæœŸåŒ–
        if world_size > 1:  # ãƒ¯ãƒ¼ãƒ«ãƒ‰ã‚µã‚¤ã‚ºãŒ1ã‚ˆã‚Šå¤§ãã„å ´åˆ
            self.model = nn.parallel.DistributedDataParallel(self.model, device_ids=[RANK], find_unused_parameters=True)  # DDPã‚’åˆæœŸåŒ–

        # Check imgsz
        gs = max(int(self.model.stride.max() if hasattr(self.model, "stride") else 32), 32)  # grid size (max stride)ã€‚ã‚°ãƒªãƒƒãƒ‰ã‚µã‚¤ã‚º
        self.args.imgsz = check_imgsz(self.args.imgsz, stride=gs, floor=gs, max_dim=1)  # ç”»åƒã‚µã‚¤ã‚ºã‚’ãƒã‚§ãƒƒã‚¯
        self.stride = gs  # for multiscale trainingã€‚ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨

        # Batch size
        if self.batch_size < 1 and RANK == -1:  # single-GPU only, estimate best batch sizeã€‚ã‚·ãƒ³ã‚°ãƒ«GPUã®å ´åˆã®ã¿ã€æœ€é©ãªãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’è¦‹ç©ã‚‚ã‚‹
            self.args.batch = self.batch_size = self.auto_batch()

        # Dataloaders
        batch_size = self.batch_size // max(world_size, 1)  # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’è¨ˆç®—
        self.train_loader = self.get_dataloader(self.trainset, batch_size=batch_size, rank=LOCAL_RANK, mode="train")  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’å–å¾—
        if RANK in {-1, 0}:  # ãƒ©ãƒ³ã‚¯ãŒ-1ã¾ãŸã¯0ã®å ´åˆ
            # Note: When training DOTA dataset, double batch size could get OOM on images with >2000 objects.
            self.test_loader = self.get_dataloader(
                self.testset, batch_size=batch_size if self.args.task == "obb" else batch_size * 2, rank=-1, mode="val"
            )  # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’å–å¾—
            self.validator = self.get_validator()  # ãƒãƒªãƒ‡ãƒ¼ã‚¿ãƒ¼ã‚’å–å¾—
            metric_keys = self.validator.metrics.keys + self.label_loss_items(prefix="val")  # ãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚­ãƒ¼ã‚’å–å¾—
            self.metrics = dict(zip(metric_keys, [0] * len(metric_keys)))  # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’åˆæœŸåŒ–
            self.ema = ModelEMA(self.model)  # EMAã‚’åˆæœŸåŒ–
            if self.args.plots:  # ãƒ—ãƒ­ãƒƒãƒˆã™ã‚‹å ´åˆ
                self.plot_training_labels()  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ©ãƒ™ãƒ«ã‚’ãƒ—ãƒ­ãƒƒãƒˆ

        # Optimizer
        self.accumulate = max(round(self.args.nbs / self.batch_size), 1)  # accumulate loss before optimizingã€‚æœ€é©åŒ–ã™ã‚‹å‰ã«æå¤±ã‚’ç´¯ç©
        weight_decay = self.args.weight_decay * self.batch_size * self.accumulate / self.args.nbs  # scale weight_decayã€‚é‡ã¿æ¸›è¡°ã‚’ã‚¹ã‚±ãƒ¼ãƒ«
        iterations = math.ceil(len(self.train_loader.dataset) / max(self.batch_size, self.args.nbs)) * self.epochs  # åå¾©å›æ•°ã‚’è¨ˆç®—
        self.optimizer = self.build_optimizer(
            model=self.model,
            name=self.args.optimizer,
            lr=self.args.lr0,
            momentum=self.args.momentum,
            decay=weight_decay,
            iterations=iterations,
        )  # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã‚’æ§‹ç¯‰
        # Scheduler
        self._setup_scheduler()  # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
        self.stopper, self.stop = EarlyStopping(patience=self.args.patience), False  # æ—©æœŸåœæ­¢ã‚’è¨­å®š
        self.resume_training(ckpt)  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å†é–‹
        self.scheduler.last_epoch = self.start_epoch - 1  # do not moveã€‚ç§»å‹•ã—ãªã„ã§ãã ã•ã„
        self.run_callbacks("on_pretrain_routine_end")  # pretrainãƒ«ãƒ¼ãƒãƒ³ã®çµ‚äº†æ™‚ã«ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’å®Ÿè¡Œ

    def _do_train(self, world_size=1):
        """Train completed, evaluate and plot if specified by arguments."""
        # å¼•æ•°ã§æŒ‡å®šã•ã‚ŒãŸå ´åˆã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Œäº†ã—ã€è©•ä¾¡ãŠã‚ˆã³ãƒ—ãƒ­ãƒƒãƒˆã—ã¾ã™ã€‚
        if world_size > 1:  # ãƒ¯ãƒ¼ãƒ«ãƒ‰ã‚µã‚¤ã‚ºãŒ1ã‚ˆã‚Šå¤§ãã„å ´åˆ
            self._setup_ddp(world_size)  # DDPã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
        self._setup_train(world_size)  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

        nb = len(self.train_loader)  # number of batchesã€‚ãƒãƒƒãƒæ•°
        nw = max(round(self.args.warmup_epochs * nb), 100) if self.args.warmup_epochs > 0 else -1  # warmup iterationsã€‚ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—åå¾©
        last_opt_step = -1  # æœ€å¾Œã®æœ€é©åŒ–ã‚¹ãƒ†ãƒƒãƒ—
        self.epoch_time = None  # ã‚¨ãƒãƒƒã‚¯æ™‚é–“
        self.epoch_time_start = time.time()  # ã‚¨ãƒãƒƒã‚¯é–‹å§‹æ™‚é–“
        self.train_time_start = time.time()  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹æ™‚é–“
        self.run_callbacks("on_train_start")  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹æ™‚ã«ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’å®Ÿè¡Œ
        LOGGER.info(  # ãƒ­ã‚°ã‚’å‡ºåŠ›
            f'Image sizes {self.args.imgsz} train, {self.args.imgsz} val\n'
            f'Using {self.train_loader.num_workers * (world_size or 1)} dataloader workers\n'
            f"Logging results to {colorstr('bold', self.save_dir)}\n"
            f'Starting training for ' + (f"{self.args.time} hours..." if self.args.time else f"{self.epochs} epochs...")
        )
        if self.args.close_mosaic:  # ãƒ¢ã‚¶ã‚¤ã‚¯ã‚’é–‰ã˜ã‚‹å ´åˆ
            base_idx = (self.epochs - self.args.close_mosaic) * nb  # åŸºæœ¬ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
            self.plot_idx.extend([base_idx, base_idx + 1, base_idx + 2])  # ãƒ—ãƒ­ãƒƒãƒˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ‹¡å¼µ
        epoch = self.start_epoch  # ã‚¨ãƒãƒƒã‚¯ã‚’é–‹å§‹ã‚¨ãƒãƒƒã‚¯ã«è¨­å®š
        self.optimizer.zero_grad()  # zero any resumed gradients to ensure stability on train startã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹æ™‚ã®å®‰å®šæ€§ã‚’ç¢ºä¿ã™ã‚‹ãŸã‚ã«ã€å†é–‹ã•ã‚ŒãŸå‹¾é…ã‚’ã‚¼ãƒ­ã«ã™ã‚‹
        while True:  # ç„¡é™ãƒ«ãƒ¼ãƒ—
            self.model.epoch = epoch  # ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‚’é€šã—ã¦_epochã‚’æ›´æ–°
            self.model.total_epochs = self.epochs  # ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‚’é€šã—ã¦_total_epochsã‚’æ›´æ–°
            self.epoch = epoch  # ã‚¨ãƒãƒƒã‚¯ã‚’è¨­å®š
            self.run_callbacks("on_train_epoch_start")
            
            with warnings.catch_warnings():  # è­¦å‘Šã‚’ã‚­ãƒ£ãƒƒãƒ
                warnings.simplefilter("ignore")  # suppress 'Detected lr_scheduler.step() before optimizer.step()'ã€‚'optimizer.stepï¼ˆï¼‰ã®å‰ã«lr_scheduler.stepï¼ˆï¼‰ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ'ã‚’æŠ‘åˆ¶
                self.scheduler.step()  # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã‚¹ãƒ†ãƒƒãƒ—

            self.model.train()  # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š
            if RANK != -1:  # DDPãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®å ´åˆ
                self.train_loader.sampler.set_epoch(epoch)  # ã‚µãƒ³ãƒ—ãƒ©ãƒ¼ã®ã‚¨ãƒãƒƒã‚¯ã‚’è¨­å®š
            pbar = enumerate(self.train_loader)  # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’åˆ—æŒ™
            # Update dataloader attributes (optional)
            if epoch == (self.epochs - self.args.close_mosaic):  # ãƒ¢ã‚¶ã‚¤ã‚¯ã‚’é–‰ã˜ã‚‹å ´åˆ
                self._close_dataloader_mosaic()  # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ãƒ¢ã‚¶ã‚¤ã‚¯ã‚’é–‰ã˜ã‚‹
                self.train_loader.reset()  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ãƒªã‚»ãƒƒãƒˆ

            if RANK in {-1, 0}:  # ãƒ©ãƒ³ã‚¯ãŒ-1ã¾ãŸã¯0ã®å ´åˆ
                LOGGER.info(self.progress_string())  # é€²è¡ŒçŠ¶æ³æ–‡å­—åˆ—ã‚’è¨˜éŒ²
                pbar = TQDM(enumerate(self.train_loader), total=nb)  # TQDMãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’åˆæœŸåŒ–
            self.tloss = None  # åˆè¨ˆæå¤±ã‚’ãƒªã‚»ãƒƒãƒˆ
            for i, batch in pbar:  # ãƒãƒƒãƒã‚’åå¾©å‡¦ç†
                self.run_callbacks("on_train_batch_start")  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒãƒƒãƒã®é–‹å§‹æ™‚ã«ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’å®Ÿè¡Œ
                # Warmup
                ni = i + nb * epoch  # åå¾©å›æ•°ã‚’è¨ˆç®—
                if ni <= nw:  # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã®å ´åˆ
                    xi = [0, nw]  # x interpã€‚xè£œé–“
                    self.accumulate = max(1, int(np.interp(ni, xi, [1, self.args.nbs / self.batch_size]).round()))  # è“„ç©æ•°ã‚’è¨ˆç®—
                    for j, x in enumerate(self.optimizer.param_groups):  # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚°ãƒ«ãƒ¼ãƒ—ã‚’åå¾©å‡¦ç†
                        # Bias lr falls from 0.1 to lr0, all other lrs rise from 0.0 to lr0
                        x["lr"] = np.interp(
                            ni, xi, [self.args.warmup_bias_lr if j == 0 else 0.0, x["initial_lr"] * self.lf(epoch)]
                        )  # å­¦ç¿’ç‡ã‚’è¨ˆç®—
                        if "momentum" in x:  # ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ãŒã‚ã‚‹å ´åˆ
                            x["momentum"] = np.interp(ni, xi, [self.args.warmup_momentum, self.args.momentum])  # ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ã‚’è¨ˆç®—

                # Forward
                with autocast(self.amp):  # è‡ªå‹•æ··åˆç²¾åº¦ã‚’ä½¿ç”¨
                    batch = self.preprocess_batch(batch)  # ãƒãƒƒãƒã‚’å‰å‡¦ç†
                    loss, self.loss_items = self.model(batch)  # ãƒ¢ãƒ‡ãƒ«ã‚’é©ç”¨
                    self.loss = loss.sum()
                    if RANK != -1:  # DDPãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®å ´åˆ
                        self.loss *= world_size  # æå¤±ã‚’ã‚¹ã‚±ãƒ¼ãƒ«
                    self.tloss = (
                        (self.tloss * i + self.loss_items) / (i + 1) if self.tloss is not None else self.loss_items
                    )  # åˆè¨ˆæå¤±ã‚’æ›´æ–°

                # Backward
                self.scaler.scale(self.loss).backward()  # æå¤±ã‚’ã‚¹ã‚±ãƒ¼ãƒ«ã—ã¦é€†ä¼æ’­

                # Optimize - https://pytorch.org/docs/master/notes/amp_examples.html
                if ni - last_opt_step >= self.accumulate:  # è“„ç©æ•°ãŒè“„ç©æ•°ä»¥ä¸Šã®å ´åˆ
                    self.optimizer_step()  # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã‚¹ãƒ†ãƒƒãƒ—
                    last_opt_step = ni  # æœ€å¾Œã®æœ€é©åŒ–ã‚¹ãƒ†ãƒƒãƒ—ã‚’æ›´æ–°

                    # Timed stopping
                    if self.args.time:  # æ™‚é–“åˆ¶é™ãŒã‚ã‚‹å ´åˆ
                        self.stop = (time.time() - self.train_time_start) > (self.args.time * 3600)  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ™‚é–“ã‚’è¶…ãˆãŸã‹ã©ã†ã‹
                        if RANK != -1:  # if DDP trainingã€‚DDPãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®å ´åˆ
                            broadcast_list = [self.stop if RANK == 0 else None]  # ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆãƒªã‚¹ãƒˆ
                            dist.broadcast_object_list(broadcast_list, 0)  # broadcast 'stop' to all ranksã€‚ã™ã¹ã¦ã®ãƒ©ãƒ³ã‚¯ã«ã€Œåœæ­¢ã€ã‚’ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆ
                            self.stop = broadcast_list[0]  # åœæ­¢çŠ¶æ…‹ã‚’æ›´æ–°
                        if self.stop:  # training time exceededã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ™‚é–“ãŒè¶…éã—ãŸå ´åˆ
                            break  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ãƒ–ãƒ¬ã‚¤ã‚¯

                # Log
                if RANK in {-1, 0}:  # ãƒ©ãƒ³ã‚¯ãŒ-1ã¾ãŸã¯0ã®å ´åˆ
                    loss_length = self.tloss.shape[0] if len(self.tloss.shape) else 1  # æå¤±ã®é•·ã•ã‚’è¨ˆç®—
                    pbar.set_description(  # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã®èª¬æ˜ã‚’è¨­å®š
                        ("%11s" * 2 + "%11.4g" * (2 + loss_length))
                        % (
                            f"{epoch + 1}/{self.epochs}",  # ã‚¨ãƒãƒƒã‚¯
                            f"{self._get_memory():.3g}G",  # ï¼ˆGBï¼‰GPUãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡
                            *(self.tloss if loss_length > 1 else torch.unsqueeze(self.tloss, 0)),  # æå¤±
                            batch["cls"].shape[0],  # ãƒãƒƒãƒã‚µã‚¤ã‚ºã€ã¤ã¾ã‚Š8
                            batch["img"].shape[-1],  # imgszã€ã¤ã¾ã‚Š640
                        )
                    )
                    self.run_callbacks("on_batch_end")  # ãƒãƒƒãƒçµ‚äº†æ™‚ã«ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’å®Ÿè¡Œ
                    if self.args.plots and ni in self.plot_idx:  # ãƒ—ãƒ­ãƒƒãƒˆã™ã‚‹å ´åˆ
                        self.plot_training_samples(batch, ni)  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚µãƒ³ãƒ—ãƒ«ã‚’ãƒ—ãƒ­ãƒƒãƒˆ

                self.run_callbacks("on_train_batch_end")  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒãƒƒãƒã®çµ‚äº†æ™‚ã«ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’å®Ÿè¡Œ

            self.lr = {f"lr/pg{ir}": x["lr"] for ir, x in enumerate(self.optimizer.param_groups)}  # for loggersã€‚ãƒ­ã‚¬ãƒ¼ç”¨
            self.run_callbacks("on_train_epoch_end")  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¨ãƒãƒƒã‚¯ã®çµ‚äº†æ™‚ã«ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’å®Ÿè¡Œ
            if RANK in {-1, 0}:  # ãƒ©ãƒ³ã‚¯ãŒ-1ã¾ãŸã¯0ã®å ´åˆ
                final_epoch = epoch + 1 >= self.epochs  # æœ€å¾Œã®epochã‹ã©ã†ã‹
                self.ema.update_attr(self.model, include=["yaml", "nc", "args", "names", "stride", "class_weights"])  # EMAã®å±æ€§ã‚’æ›´æ–°


                # Validation
                if self.args.val or final_epoch or self.stopper.possible_stop or self.stop:
                     self.metrics, self.fitness = self.validate()  # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ
                self.save_metrics(metrics={**self.label_loss_items(self.tloss), **self.metrics, **self.lr})  # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ä¿å­˜
                self.stop |= self.stopper(epoch + 1, self.fitness) or final_epoch  # æ—©æœŸåœæ­¢ã‚’é©ç”¨
                if self.args.time:  # æ™‚é–“åˆ¶é™ãŒã‚ã‚‹å ´åˆ
                    self.stop |= (time.time() - self.train_time_start) > (self.args.time * 3600)  # æ™‚é–“ã‚’è¶…éã—ãŸã‹ã©ã†ã‹

                # Save model
                if self.args.save or final_epoch:  # ä¿å­˜ã¾ãŸã¯æœ€çµ‚ã‚¨ãƒãƒƒã‚¯ã®å ´åˆ
                    self.save_model()  # ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
                    self.run_callbacks("on_model_save")  # ãƒ¢ãƒ‡ãƒ«ä¿å­˜æ™‚ã«ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’å®Ÿè¡Œ

            # Scheduler
            t = time.time()  # ç¾åœ¨æ™‚åˆ»ã‚’å–å¾—
            self.epoch_time = t - self.epoch_time_start  # ã‚¨ãƒãƒƒã‚¯æ™‚é–“ã‚’è¨ˆç®—
            self.epoch_time_start = t  # ã‚¨ãƒãƒƒã‚¯é–‹å§‹æ™‚é–“ã‚’æ›´æ–°
            if self.args.time:  # æ™‚é–“åˆ¶é™ãŒã‚ã‚‹å ´åˆ
                mean_epoch_time = (t - self.train_time_start) / (epoch - self.start_epoch + 1)  # å¹³å‡ã‚¨ãƒãƒƒã‚¯æ™‚é–“ã‚’è¨ˆç®—
                self.epochs = self.args.epochs = math.ceil(self.args.time * 3600 / mean_epoch_time)  # ã‚¨ãƒãƒƒã‚¯æ•°ã‚’å†è¨ˆç®—
                self._setup_scheduler()  # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’å†ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
                self.scheduler.last_epoch = self.epoch  # do not moveã€‚ç§»å‹•ã—ãªã„ã§ãã ã•ã„
                self.stop |= epoch >= self.epochs  # stop if exceeded epochsã€‚ã‚¨ãƒãƒƒã‚¯ã‚’è¶…éã—ãŸå ´åˆã«åœæ­¢
            self.run_callbacks("on_fit_epoch_end")  # ãƒ•ã‚£ãƒƒãƒˆã‚¨ãƒãƒƒã‚¯çµ‚äº†æ™‚ã«ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’å®Ÿè¡Œ
            if self._get_memory(fraction=True) > 0.5:
                self._clear_memory()  # ãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢

            # Early Stopping
            if RANK != -1:  # if DDP trainingã€‚DDPãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®å ´åˆ
                broadcast_list = [self.stop if RANK == 0 else None]  # ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆãƒªã‚¹ãƒˆ
                dist.broadcast_object_list(broadcast_list, 0)  # broadcast 'stop' to all ranksã€‚ã™ã¹ã¦ã®ãƒ©ãƒ³ã‚¯ã«ã€Œåœæ­¢ã€ã‚’ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆ
                self.stop = broadcast_list[0]  # åœæ­¢çŠ¶æ…‹ã‚’æ›´æ–°
            if self.stop:  # åœæ­¢ã™ã‚‹å ´åˆ
                break  # must break all DDP ranksã€‚ã™ã¹ã¦ã®DDPãƒ©ãƒ³ã‚¯ã‚’ä¸­æ–­ã™ã‚‹å¿…è¦ãŒã‚ã‚‹
            epoch += 1  # ã‚¨ãƒãƒƒã‚¯ã‚’ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ãƒˆ

        if RANK in {-1, 0}:  # ãƒ©ãƒ³ã‚¯ãŒ-1ã¾ãŸã¯0ã®å ´åˆ
            # Do final val with best.pt
            seconds = time.time() - self.train_time_start  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ™‚é–“ã‚’è¨ˆç®—
            LOGGER.info(f"\n{epoch - self.start_epoch + 1} epochs completed in {seconds / 3600:.3f} hours.")  # ãƒ­ã‚°ã‚’å‡ºåŠ›
            self.final_eval()  # æœ€çµ‚è©•ä¾¡ã‚’å®Ÿè¡Œ
            if self.args.plots:  # ãƒ—ãƒ­ãƒƒãƒˆã™ã‚‹å ´åˆ
                self.plot_metrics()  # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
            self.run_callbacks("on_train_end")  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°çµ‚äº†æ™‚ã«ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’å®Ÿè¡Œ
        self._clear_memory()  # ãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢
        unset_deterministic()
        self.run_callbacks("teardown")  # ãƒ†ã‚£ã‚¢ãƒ€ã‚¦ãƒ³æ™‚ã«ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’å®Ÿè¡Œ

    def auto_batch(self, max_num_obj=0):
        """Calculate optimal batch size based on model and device memory constraints."""
        return check_train_batch_size(
            model=self.model,
            imgsz=self.args.imgsz,
            amp=self.amp,
            batch=self.batch_size,
            max_num_obj=max_num_obj,
        )  # returns batch size
    def _get_memory(self, fraction=False):
        """Get accelerator memory utilization in GB."""
        # GBå˜ä½ã®ã‚¢ã‚¯ã‚»ãƒ©ãƒ¬ãƒ¼ã‚¿ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ã‚’å–å¾—ã—ã¾ã™ã€‚
        memory, total = 0, 0
        if self.device.type == "mps":
            memory = torch.mps.driver_allocated_memory()
            if fraction:
                return __import__("psutil").virtual_memory().percent / 100
        elif self.device.type != "cpu":
            memory = torch.cuda.memory_reserved()
            if fraction:
                total = torch.cuda.get_device_properties(self.device).total_memory
        return ((memory / total) if total > 0 else 0) if fraction else (memory / 2**30)

    def _clear_memory(self):
        """Clear accelerator memory on different platforms."""
        # ç•°ãªã‚‹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã§ã‚¢ã‚¯ã‚»ãƒ©ãƒ¬ãƒ¼ã‚¿ãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢ã—ã¾ã™ã€‚
        gc.collect()  # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ
        if self.device.type == "mps":  # ãƒ‡ãƒã‚¤ã‚¹ã‚¿ã‚¤ãƒ—ãŒmpsã®å ´åˆ
            torch.mps.empty_cache()  # MPSã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢
        elif self.device.type == "cpu":  # ãƒ‡ãƒã‚¤ã‚¹ã‚¿ã‚¤ãƒ—ãŒcpuã®å ´åˆ
            return  # ä½•ã‚‚ã—ãªã„
        else:  # ãã‚Œä»¥å¤–ã®å ´åˆ
            torch.cuda.empty_cache()  # CUDAã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢

    def read_results_csv(self):
        """Read results.csv into a dict using pandas."""
        # pandasã‚’ä½¿ç”¨ã—ã¦results.csvã‚’dictã«èª­ã¿è¾¼ã¿ã¾ã™ã€‚
        import pandas as pd  # scope for faster 'import ultralytics'ã€‚ã‚ˆã‚Šé«˜é€Ÿãªã€Œimport ultralyticsã€ã®ã‚¹ã‚³ãƒ¼ãƒ—

        return pd.read_csv(self.csv).to_dict(orient="list")  # csvã‚’èª­ã¿å–ã£ã¦è¾æ›¸ã«å¤‰æ›
    def _model_train(self):
        """Set model in training mode."""
        self.model.train()
        # Freeze BN stat
        for n, m in self.model.named_modules():
            if any(filter(lambda f: f in n, self.freeze_layer_names)) and isinstance(m, nn.BatchNorm2d):
                m.eval()
    def save_model(self):
        """Save model training checkpoints with additional metadata."""
        # è¿½åŠ ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜ã—ã¾ã™ã€‚
        import io  # ioãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

        # Serialize ckpt to a byte buffer once (faster than repeated torch.save() calls)
        buffer = io.BytesIO()  # ãƒã‚¤ãƒˆIOãƒãƒƒãƒ•ã‚¡ã‚’ä½œæˆ
        torch.save(  # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ã‚·ãƒªã‚¢ãƒ«åŒ–
            {
                "epoch": self.epoch,  # ã‚¨ãƒãƒƒã‚¯
                "best_fitness": self.best_fitness,  # æœ€é«˜ã®é©åˆåº¦
                "model": None,  # resume and final checkpoints derive from EMAã€‚ãƒ¬ã‚¸ãƒ¥ãƒ¼ãƒ ã¨æœ€çµ‚ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¯EMAã‹ã‚‰æ´¾ç”Ÿ
                "ema": deepcopy(self.ema.ema).half(),  # EMA
                "updates": self.ema.updates,  # ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ
                "optimizer": convert_optimizer_state_dict_to_fp16(deepcopy(self.optimizer.state_dict())),  # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶
                "train_args": vars(self.args),  # save as dictã€‚è¾æ›¸ã¨ã—ã¦ä¿å­˜
                "train_metrics": {**self.metrics, **{"fitness": self.fitness}},  # train metricsã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¡ãƒˆãƒªã‚¯ã‚¹
                "train_results": self.read_results_csv(),  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°çµæœ
                "date": datetime.now().isoformat(),  # ç¾åœ¨æ—¥æ™‚
                "license": "AGPL-3.0 (https://ultralytics.com/license)",  # ãƒ©ã‚¤ã‚»ãƒ³ã‚¹
                "docs": "https://docs.ultralytics.com",  # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
            },
            buffer,  # ãƒãƒƒãƒ•ã‚¡ã«ä¿å­˜
        )
        serialized_ckpt = buffer.getvalue()  # get the serialized content to saveã€‚ä¿å­˜ã™ã‚‹ã‚·ãƒªã‚¢ãƒ«åŒ–ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—

        # Save checkpoints
        self.last.write_bytes(serialized_ckpt)  # save last.ptã€‚last.ptã‚’ä¿å­˜
        if self.best_fitness == self.fitness:  # æœ€é«˜ã®é©åˆåº¦ã®å ´åˆ
            self.best.write_bytes(serialized_ckpt)  # save best.ptã€‚best.ptã‚’ä¿å­˜
        if (self.save_period > 0) and (self.epoch % self.save_period == 0):  # ä¿å­˜æœŸé–“ã®å ´åˆ
            (self.wdir / f"epoch{self.epoch}.pt").write_bytes(serialized_ckpt)  # save epoch, i.e. 'epoch3.pt'ã€‚epochã‚’ä¿å­˜

    def get_dataset(self):
        """
        Get train, val path from data dict if it exists.

        Returns None if data format is not recognized.
        """
        # å­˜åœ¨ã™ã‚‹å ´åˆã€ãƒ‡ãƒ¼ã‚¿è¾æ›¸ã‹ã‚‰trainã€valãƒ‘ã‚¹ã‚’å–å¾—ã—ã¾ã™ã€‚
        # ãƒ‡ãƒ¼ã‚¿å½¢å¼ãŒèªè­˜ã•ã‚Œãªã„å ´åˆã¯Noneã‚’è¿”ã—ã¾ã™ã€‚
        try:
            if self.args.task == "classify":  # åˆ†é¡ã‚¿ã‚¹ã‚¯ã®å ´åˆ
                data = check_cls_dataset(self.args.data)  # åˆ†é¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒã‚§ãƒƒã‚¯
            elif self.args.data.split(".")[-1] in {"yaml", "yml"} or self.args.task in {
                "detect",
                "segment",
                "pose",
                "obb",
            }:  # ãƒ‡ãƒ¼ã‚¿å½¢å¼ãŒyamlã¾ãŸã¯ymlã®å ´åˆ
                data = check_det_dataset(self.args.data)  # æ¤œå‡ºãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒã‚§ãƒƒã‚¯
                if "yaml_file" in data:  # yaml_fileãŒã‚ã‚‹å ´åˆ
                    self.args.data = data["yaml_file"]  # for validating 'yolo train data=url.zip' usageã€‚ 'yolo train data=url.zip'ã®ä½¿ç”¨æ³•ã‚’æ¤œè¨¼ã™ã‚‹ãŸã‚
        except Exception as e:  # ä¾‹å¤–ãŒç™ºç”Ÿã—ãŸå ´åˆ
            raise RuntimeError(emojis(f"Dataset '{clean_url(self.args.data)}' error âŒ {e}")) from e  # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç”Ÿæˆ
        self.data = data  # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ã‚¢ã‚¿ãƒƒãƒ
        if self.args.single_cls:
            LOGGER.info("Overriding class names with single class.")
            self.data["names"] = {0: "item"}
            self.data["nc"] = 1
        return data["train"], data.get("val") or data.get("test")  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ã‚¹ã¨æ¤œè¨¼ãƒ‘ã‚¹ã‚’è¿”ã™

    def setup_model(self):
        """Load/create/download model for any task."""
        # ã‚¿ã‚¹ã‚¯ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰/ä½œæˆ/ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚
        if isinstance(self.model, torch.nn.Module):  # if model is loaded beforehand. No setup neededã€‚ãƒ¢ãƒ‡ãƒ«ãŒäº‹å‰ã«ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹å ´åˆã€‚ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã¯ä¸è¦
            return  # ä½•ã‚‚ã—ãªã„

        cfg, weights = self.model, None  # è¨­å®šã¨é‡ã¿ã‚’åˆæœŸåŒ–
        ckpt = None  # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’åˆæœŸåŒ–
        if str(self.model).endswith(".pt"):  # ãƒ¢ãƒ‡ãƒ«ãŒ.ptã§çµ‚ã‚ã‚‹å ´åˆ
            weights, ckpt = attempt_load_one_weight(self.model)  # ãƒ¢ãƒ‡ãƒ«ã¨ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ãƒ­ãƒ¼ãƒ‰
            cfg = weights.yaml  # è¨­å®šã‚’ãƒ­ãƒ¼ãƒ‰
        elif isinstance(self.args.pretrained, (str, Path)):  # äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆ
            weights, _ = attempt_load_one_weight(self.args.pretrained)  # äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
        self.model = self.get_model(cfg=cfg, weights=weights, verbose=RANK == -1)  # calls Model(cfg, weights)ã€‚Modelï¼ˆcfgã€weightsï¼‰ã‚’å‘¼ã³å‡ºã™
        return ckpt  # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’è¿”ã™

    def optimizer_step(self):
        """Perform a single step of the training optimizer with gradient clipping and EMA update."""
        # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã¨EMAæ›´æ–°ã‚’ä½¿ç”¨ã—ã¦ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã®å˜ä¸€ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
        self.scaler.unscale_(self.optimizer)  # unscale gradientsã€‚å‹¾é…ã‚’ã‚¢ãƒ³ ã‚¹ã‚±ãƒ¼ãƒ«
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=10.0)  # clip gradientsã€‚å‹¾é…ã‚’ã‚¯ãƒªãƒƒãƒ—
        self.scaler.step(self.optimizer)  # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã‚¹ãƒ†ãƒƒãƒ—
        self.scaler.update()  # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã‚’æ›´æ–°
        self.optimizer.zero_grad()  # å‹¾é…ã‚’ã‚¼ãƒ­
        if self.ema:  # EMAãŒã‚ã‚‹å ´åˆ
            self.ema.update(self.model)  # EMAã‚’æ›´æ–°

    def preprocess_batch(self, batch):
        """Allows custom preprocessing model inputs and ground truths depending on task type."""
        # ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦ã€ã‚«ã‚¹ã‚¿ãƒ å‰å‡¦ç†ãƒ¢ãƒ‡ãƒ«å…¥åŠ›ã¨ã‚°ãƒ©ãƒ³ãƒ‰ãƒˆã‚¥ãƒ«ãƒ¼ã‚¹ã‚’è¨±å¯ã—ã¾ã™ã€‚
        return batch  # ãƒãƒƒãƒã‚’è¿”ã™

    def validate(self):
        """
        Runs validation on test set using self.validator.

        The returned dict is expected to contain "fitness" key.
        """
        # self.validatorã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§æ¤œè¨¼ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
        # è¿”ã•ã‚Œã‚‹dictã¯ã€ã€Œfitnessã€ã‚­ãƒ¼ã‚’å«ã‚€ã“ã¨ãŒæœŸå¾…ã•ã‚Œã¾ã™ã€‚
        metrics = self.validator(self)  # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ
        fitness = metrics.pop("fitness", -self.loss.detach().cpu().numpy())  # use loss as fitness measure if not foundã€‚é©åˆåº¦ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€æå¤±ã‚’é©åˆåº¦ã®å°ºåº¦ã¨ã—ã¦ä½¿ç”¨
        if not self.best_fitness or self.best_fitness < fitness: # æœ€é«˜ã®é©åˆåº¦ãŒãªã„ã€ã¾ãŸã¯æœ€é«˜ã®é©åˆåº¦ã‚ˆã‚Šé©åˆåº¦ãŒé«˜ã„å ´åˆ
            print('\033[34m' + f"best fitness updated to {self.epoch + 1}" + '\033[0m') # ãƒ­ã‚°ã‚’å‡ºåŠ›
            self.best_fitness = fitness  # æœ€é«˜ã®é©åˆåº¦ã‚’æ›´æ–°
        return metrics, fitness  # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¨é©åˆåº¦ã‚’è¿”ã™

    def get_model(self, cfg=None, weights=None, verbose=True):
        """Get model and raise NotImplementedError for loading cfg files."""
        # ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—ã—ã€cfgãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¯¾ã—ã¦NotImplementedErrorã‚’ç™ºç”Ÿã•ã›ã¾ã™ã€‚
        raise NotImplementedError("This task trainer doesn't support loading cfg files")  # ã‚¨ãƒ©ãƒ¼ã‚’ç™ºç”Ÿ

    def get_validator(self):
        """Returns a NotImplementedError when the get_validator function is called."""
        # get_validatoré–¢æ•°ãŒå‘¼ã³å‡ºã•ã‚ŒãŸã¨ãã«NotImplementedErrorã‚’è¿”ã—ã¾ã™ã€‚
        raise NotImplementedError("get_validator function not implemented in trainer")  # ã‚¨ãƒ©ãƒ¼ã‚’ç™ºç”Ÿ

    def get_dataloader(self, dataset_path, batch_size=16, rank=0, mode="train"):
        """Returns dataloader derived from torch.data.Dataloader."""
        # torch.data.Dataloaderã‹ã‚‰æ´¾ç”Ÿã—ãŸdataloaderã‚’è¿”ã—ã¾ã™ã€‚
        raise NotImplementedError("get_dataloader function not implemented in trainer")  # ã‚¨ãƒ©ãƒ¼ã‚’ç™ºç”Ÿ

    def build_dataset(self, img_path, mode="train", batch=None):
        """Build dataset."""
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚
        raise NotImplementedError("build_dataset function not implemented in trainer")  # ã‚¨ãƒ©ãƒ¼ã‚’ç™ºç”Ÿ

    def label_loss_items(self, loss_items=None, prefix="train"):
        """
        Returns a loss dict with labelled training loss items tensor.

        Note:
            This is not needed for classification but necessary for segmentation & detection
        """
        # ãƒ©ãƒ™ãƒ«ä»˜ã‘ã•ã‚ŒãŸãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æå¤±ã‚¢ã‚¤ãƒ†ãƒ ãƒ†ãƒ³ã‚½ãƒ«ã‚’å«ã‚€æå¤±dictã‚’è¿”ã—ã¾ã™ã€‚
        #
        # æ³¨ï¼š
        #     ã“ã‚Œã¯åˆ†é¡ã«ã¯å¿…è¦ã‚ã‚Šã¾ã›ã‚“ãŒã€ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã¨æ¤œå‡ºã«ã¯å¿…è¦ã§ã™
        return {"loss": loss_items} if loss_items is not None else ["loss"]  # æå¤±ã‚¢ã‚¤ãƒ†ãƒ ãŒã‚ã‚‹å ´åˆã¯æå¤±ãƒ‡ã‚£ã‚¯ã‚·ãƒ§ãƒŠãƒªã‚’è¿”ã—ã€ãã‚Œä»¥å¤–ã®å ´åˆã¯ã€Œlossã€ã‚’è¿”ã—ã¾ã™

    def set_model_attributes(self):
        """To set or update model parameters before training."""
        # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å‰ã«ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®šã¾ãŸã¯æ›´æ–°ã—ã¾ã™ã€‚
        self.model.names = self.data["names"]  # ã‚¯ãƒ©ã‚¹åã‚’è¨­å®š

    def build_targets(self, preds, targets):
        """Builds target tensors for training YOLO model."""
        # YOLOãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ãŸã‚ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ†ãƒ³ã‚½ãƒ«ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚
        pass  # ä½•ã‚‚ã—ãªã„

    def progress_string(self):
        """Returns a string describing training progress."""
        # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®é€²è¡ŒçŠ¶æ³ã‚’èª¬æ˜ã™ã‚‹æ–‡å­—åˆ—ã‚’è¿”ã—ã¾ã™ã€‚
        return ""  # ç©ºã®æ–‡å­—åˆ—ã‚’è¿”ã™

    # TODO: may need to put these following functions into callback
    def plot_training_samples(self, batch, ni):
        """Plots training samples during YOLO training."""
        # YOLOãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚µãƒ³ãƒ—ãƒ«ã‚’ãƒ—ãƒ­ãƒƒãƒˆã—ã¾ã™ã€‚
        pass  # ä½•ã‚‚ã—ãªã„

    def plot_training_labels(self):
        """Plots training labels for YOLO model."""
        # YOLOãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ©ãƒ™ãƒ«ã‚’ãƒ—ãƒ­ãƒƒãƒˆã—ã¾ã™ã€‚
        pass  # ä½•ã‚‚ã—ãªã„

    def save_metrics(self, metrics):
        """Saves training metrics to a CSV file."""
        # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã™ã€‚
        keys, vals = list(metrics.keys()), list(metrics.values())  # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ã‚­ãƒ¼ã¨å€¤ã‚’å–å¾—
        n = len(metrics) + 2  # number of colsã€‚åˆ—æ•°
        s = "" if self.csv.exists() else (("%s," * n % tuple(["epoch", "time"] + keys)).rstrip(",") + "\n")  # headerã€‚ãƒ˜ãƒƒãƒ€ãƒ¼
        t = time.time() - self.train_time_start  # æ™‚é–“ã‚’è¨ˆç®—
        with open(self.csv, "a", encoding="utf-8") as f:  # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ã
            f.write(s + ("%.6g," * n % tuple([self.epoch + 1, t] + vals)).rstrip(",") + "\n")  # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ä¿å­˜

    def plot_metrics(self):
        """Plot and display metrics visually."""
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒ—ãƒ­ãƒƒãƒˆã—ã¦è¦–è¦šçš„ã«è¡¨ç¤ºã—ã¾ã™ã€‚
        pass  # ä½•ã‚‚ã—ãªã„

    def on_plot(self, name, data=None):
        """Registers plots (e.g. to be consumed in callbacks)."""
        # ãƒ—ãƒ­ãƒƒãƒˆã‚’ç™»éŒ²ã—ã¾ã™ï¼ˆä¾‹ï¼šã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§ä½¿ç”¨ã™ã‚‹ãŸã‚ï¼‰ã€‚
        path = Path(name)  # ãƒ‘ã‚¹ã‚’ç”Ÿæˆ
        self.plots[path] = {"data": data, "timestamp": time.time()}  # ãƒ—ãƒ­ãƒƒãƒˆã‚’ç™»éŒ²

    def final_eval(self):
        """Performs final evaluation and validation for object detection YOLO model."""
        # ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæ¤œå‡ºYOLOãƒ¢ãƒ‡ãƒ«ã®æœ€çµ‚è©•ä¾¡ã¨æ¤œè¨¼ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
        ckpt = {}  # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’åˆæœŸåŒ–
        for f in self.last, self.best:  # æœ€å¾Œã®ãƒ¢ãƒ‡ãƒ«ã¨æœ€é«˜ã®ãƒ¢ãƒ‡ãƒ«ã‚’åå¾©å‡¦ç†
            if f.exists():  # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆ
                if f is self.last:  # æœ€å¾Œã®ãƒ¢ãƒ‡ãƒ«ã®å ´åˆ
                    ckpt = strip_optimizer(f)  # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã‚’å‰Šé™¤
                elif f is self.best:  # æœ€é«˜ã®ãƒ¢ãƒ‡ãƒ«ã®å ´åˆ
                    k = "train_results"  # update best.pt train_metrics from last.ptã€‚last.ptã‹ã‚‰best.pt train_metricsã‚’æ›´æ–°
                    strip_optimizer(f, updates={k: ckpt[k]} if k in ckpt else None)  # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã‚’å‰Šé™¤
                    LOGGER.info(f"\nValidating {f}...")  # ãƒ­ã‚°ã‚’å‡ºåŠ›
                    self.validator.args.plots = self.args.plots  # ãƒ—ãƒ­ãƒƒãƒˆå¼•æ•°ã‚’è¨­å®š
                    self.metrics = self.validator(model=f)  # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ
                    self.metrics.pop("fitness", None)  # é©åˆåº¦ã‚’å‰Šé™¤
                    self.run_callbacks("on_fit_epoch_end")  # ãƒ•ã‚£ãƒƒãƒˆã‚¨ãƒãƒƒã‚¯çµ‚äº†æ™‚ã«ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’å®Ÿè¡Œ

    def check_resume(self, overrides):
        """Check if resume checkpoint exists and update arguments accordingly."""
        # ãƒ¬ã‚¸ãƒ¥ãƒ¼ãƒ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒå­˜åœ¨ã™ã‚‹ã‹ã©ã†ã‹ã‚’ç¢ºèªã—ã€ãã‚Œã«å¿œã˜ã¦å¼•æ•°ã‚’æ›´æ–°ã—ã¾ã™ã€‚
        resume = self.args.resume  # ãƒ¬ã‚¸ãƒ¥ãƒ¼ãƒ å¼•æ•°ã‚’å–å¾—
        if resume:  # ãƒ¬ã‚¸ãƒ¥ãƒ¼ãƒ ãŒTrueã®å ´åˆ
            try:
                exists = isinstance(resume, (str, Path)) and Path(resume).exists()  # ãƒ¬ã‚¸ãƒ¥ãƒ¼ãƒ ãŒå­˜åœ¨ã™ã‚‹ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯
                last = Path(check_file(resume) if exists else get_latest_run())  # æœ€å¾Œã®å®Ÿè¡Œã‚’å–å¾—

                # Check that resume data YAML exists, otherwise strip to force re-download of dataset
                ckpt_args = attempt_load_weights(last).args  # ãƒ¬ã‚¸ãƒ¥ãƒ¼ãƒ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆå¼•æ•°ã‚’ãƒ­ãƒ¼ãƒ‰
                if not isinstance(ckpt_args["data"], dict) and not Path(ckpt_args["data"]).exists():
                    ckpt_args["data"] = self.args.data  # ãƒ‡ãƒ¼ã‚¿YAMLã‚’æ›´æ–°

                resume = True  # ãƒ¬ã‚¸ãƒ¥ãƒ¼ãƒ ã‚’Trueã«è¨­å®š
                self.args = get_cfg(ckpt_args)  # è¨­å®šã‚’å–å¾—
                self.args.model = self.args.resume = str(last)  # æœ€å¾Œã®ãƒ¢ãƒ‡ãƒ«ã§å†è¨­å®š
                for k in (
                    "imgsz",
                    "batch",
                    "device",
                    "close_mosaic",
                ):  # allow arg updates to reduce memory or update device on resumeã€‚ãƒ¡ãƒ¢ãƒªã‚’å‰Šæ¸›ã—ãŸã‚Šã€å†é–‹æ™‚ã«ãƒ‡ãƒã‚¤ã‚¹ã‚’æ›´æ–°ã—ãŸã‚Šã™ã‚‹ãŸã‚ã®å¼•æ•°æ›´æ–°ã‚’è¨±å¯
                    if k in overrides:  # å¼•æ•°ãŒã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ã«å«ã¾ã‚Œã¦ã„ã‚‹å ´åˆ
                        setattr(self.args, k, overrides[k])  # å¼•æ•°ã‚’è¨­å®š

            except Exception as e:  # ä¾‹å¤–ãŒç™ºç”Ÿã—ãŸå ´åˆ
                raise FileNotFoundError(
                    "Resume checkpoint not found. Please pass a valid checkpoint to resume from, "
                    "i.e. 'yolo train resume model=path/to/last.pt'"
                ) from e  # ä¾‹å¤–ã‚’ç™ºç”Ÿ
        self.resume = resume  # ãƒ¬ã‚¸ãƒ¥ãƒ¼ãƒ ã‚’è¨­å®š

    def resume_training(self, ckpt):
        """Resume YOLO training from given epoch and best fitness."""
        # æŒ‡å®šã•ã‚ŒãŸã‚¨ãƒãƒƒã‚¯ã¨æœ€é«˜ã®é©åˆåº¦ã‹ã‚‰YOLOãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å†é–‹ã—ã¾ã™ã€‚
        if ckpt is None or not self.resume:  # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒãªã„å ´åˆã€ã¾ãŸã¯å†é–‹ã—ãªã„å ´åˆ
            return  # ä½•ã‚‚ã—ãªã„
        best_fitness = 0.0  # æœ€é«˜ã®é©åˆåº¦ã‚’åˆæœŸåŒ–
        start_epoch = ckpt.get("epoch", -1) + 1  # é–‹å§‹ã‚¨ãƒãƒƒã‚¯ã‚’å–å¾—
        if ckpt.get("optimizer", None) is not None:  # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãŒã‚ã‚‹å ´åˆ
            self.optimizer.load_state_dict(ckpt["optimizer"])  # optimizerã€‚ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã‚’ãƒ­ãƒ¼ãƒ‰
            best_fitness = ckpt["best_fitness"]  # æœ€é«˜ã®é©åˆåº¦ã‚’å–å¾—
        if self.ema and ckpt.get("ema"):  # EMAãŒã‚ã‚Šã€ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã«EMAãŒã‚ã‚‹å ´åˆ
            self.ema.ema.load_state_dict(ckpt["ema"].float().state_dict())  # EMAã‚’ãƒ­ãƒ¼ãƒ‰
            self.ema.updates = ckpt["updates"]  # ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆã‚’å–å¾—
        assert start_epoch > 0, (  # é–‹å§‹ã‚¨ãƒãƒƒã‚¯ãŒ0ã‚ˆã‚Šå¤§ãã„ã“ã¨ã‚’ç¢ºèª
            f"{self.args.model} training to {self.epochs} epochs is finished, nothing to resume.\n"
            f"Start a new training without resuming, i.e. 'yolo train model={self.args.model}'"
        )
        LOGGER.info(f"Resuming training {self.args.model} from epoch {start_epoch + 1} to {self.epochs} total epochs")  # ãƒ­ã‚°ã‚’å‡ºåŠ›
        if self.epochs < start_epoch:
            LOGGER.info(
                f"{self.model} has been trained for {ckpt['epoch']} epochs. Fine-tuning for {self.epochs} more epochs."
            )
            self.epochs += ckpt["epoch"]  # finetune additional epochs
        self.best_fitness = best_fitness  # æœ€é«˜ã®é©åˆåº¦ã‚’è¨­å®š
        self.start_epoch = start_epoch  # é–‹å§‹ã‚¨ãƒãƒƒã‚¯ã‚’è¨­å®š
        if start_epoch > (self.epochs - self.args.close_mosaic):  # é–‹å§‹ã‚¨ãƒãƒƒã‚¯ãŒclose_mosaicã‚’è¶…ãˆã‚‹å ´åˆ
            self._close_dataloader_mosaic()  # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ãƒ¢ã‚¶ã‚¤ã‚¯ã‚’é–‰ã˜ã‚‹


    def _close_dataloader_mosaic(self):
        """Update dataloaders to stop using mosaic augmentation."""
        # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’æ›´æ–°ã—ã¦ã€ãƒ¢ã‚¶ã‚¤ã‚¯æ‹¡å¼µã®ä½¿ç”¨ã‚’åœæ­¢ã—ã¾ã™ã€‚
        if hasattr(self.train_loader.dataset, "mosaic"):  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã«ãƒ¢ã‚¶ã‚¤ã‚¯ãŒã‚ã‚‹å ´åˆ
            self.train_loader.dataset.mosaic = False  # ãƒ¢ã‚¶ã‚¤ã‚¯ã‚’ç„¡åŠ¹åŒ–
        if hasattr(self.train_loader.dataset, "close_mosaic"):  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã«close_mosaicãŒã‚ã‚‹å ´åˆ
            LOGGER.info("Closing dataloader mosaic")  # ãƒ­ã‚°ã‚’å‡ºåŠ›
            self.train_loader.dataset.close_mosaic(hyp=copy(self.args))  # ãƒ¢ã‚¶ã‚¤ã‚¯ã‚’é–‰ã˜ã‚‹

    def build_optimizer(self, model, name="auto", lr=0.001, momentum=0.9, decay=1e-5, iterations=1e5):
        # æŒ‡å®šã•ã‚ŒãŸã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶åã€å­¦ç¿’ç‡ã€ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ã€é‡ã¿æ¸›è¡°ã€ãŠã‚ˆã³åå¾©å›æ•°ã«åŸºã¥ã„ã¦ã€æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚
        #
        # å¼•æ•°ï¼š
        #     model (torch.nn.Module): ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã‚’æ§‹ç¯‰ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã€‚
        #     name (str, optional): ä½¿ç”¨ã™ã‚‹ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã®åå‰ã€‚ 'auto'ã®å ´åˆã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã¯åå¾©å›æ•°ã«åŸºã¥ã„ã¦é¸æŠã•ã‚Œã¾ã™ã€‚
        #         ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼šã€Œautoã€ã€‚
        #     lr (float, optional): ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã®å­¦ç¿’ç‡ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼š0.001ã€‚
        #     momentum (float, optional): ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã®ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼š0.9ã€‚
        #     decay (float, optional): ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã®é‡ã¿æ¸›è¡°ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼š1e-5ã€‚
        #     iterations (float, optional): åå¾©å›æ•°ã€‚nameãŒ 'auto'ã®å ´åˆã«ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã‚’æ±ºå®šã—ã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼š1e5ã€‚
        #
        # æˆ»ã‚Šå€¤ï¼š
        #     (torch.optim.Optimizer): æ§‹ç¯‰ã•ã‚ŒãŸã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã€‚
        g = [], [], []  # optimizer parameter groupsã€‚ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚°ãƒ«ãƒ¼ãƒ—
        bn = tuple(v for k, v in nn.__dict__.items() if "Norm" in k)  # normalization layers, i.e. BatchNorm2d()ã€‚æ­£è¦åŒ–ãƒ¬ã‚¤ãƒ¤ãƒ¼
        if name == "auto":  # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶åãŒautoã®å ´åˆ
            LOGGER.info(
                f"{colorstr('optimizer:')} 'optimizer=auto' found, "
                f"ignoring 'lr0={self.args.lr0}' and 'momentum={self.args.momentum}' and "
                f"determining best 'optimizer', 'lr0' and 'momentum' automatically... "
            )  # ãƒ­ã‚°ã‚’å‡ºåŠ›
            nc = self.data.get("nc", 10)  # number of classesã€‚ã‚¯ãƒ©ã‚¹æ•°
            lr_fit = round(0.002 * 5 / (4 + nc), 6)  # lr0 fit equation to 6 decimal placesã€‚6æ¡ã®10é€²æ•°ã¸ã®lr0é©åˆæ–¹ç¨‹å¼
            name, lr, momentum = ("SGD", 0.01, 0.9) if iterations > 10000 else ("AdamW", lr_fit, 0.9)  # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã€å­¦ç¿’ç‡ã€ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ã‚’è¨­å®š
            self.args.warmup_bias_lr = 0.0  # no higher than 0.01 for Adamã€‚Adamã®å ´åˆã¯0.01ã‚’è¶…ãˆãªã„

        for module_name, module in model.named_modules():  # ãƒ¢ãƒ‡ãƒ«ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’åå¾©å‡¦ç†
            for param_name, param in module.named_parameters(recurse=False):  # ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’åå¾©å‡¦ç†
                fullname = f"{module_name}.{param_name}" if module_name else param_name  # å®Œå…¨åã‚’å–å¾—
                if "bias" in fullname:  # bias (no decay)ã€‚ãƒã‚¤ã‚¢ã‚¹ï¼ˆæ¸›è¡°ãªã—ï¼‰
                    g[2].append(param)  # ãƒã‚¤ã‚¢ã‚¹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒªã‚¹ãƒˆã«è¿½åŠ 
                elif isinstance(module, bn):  # weight (no decay)ã€‚é‡ã¿ï¼ˆæ¸›è¡°ãªã—ï¼‰
                    g[1].append(param)  # æ­£è¦åŒ–ãƒ¬ã‚¤ãƒ¤ã‚’ãƒªã‚¹ãƒˆã«è¿½åŠ 
                else:  # weight (with decay)ã€‚é‡ã¿ï¼ˆæ¸›è¡°ã‚ã‚Šï¼‰
                    g[0].append(param)  # ãã®ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒªã‚¹ãƒˆã«è¿½åŠ 
        optimizers = {"Adam", "Adamax", "AdamW", "NAdam", "RAdam", "RMSProp", "SGD", "auto"}
        name = {x.lower(): x for x in optimizers}.get(name.lower())
        if name in {"Adam", "Adamax", "AdamW", "NAdam", "RAdam"}:  # ã‚¢ãƒ€ãƒ ç³»ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã®å ´åˆ
            optimizer = getattr(optim, name, optim.Adam)(g[2], lr=lr, betas=(momentum, 0.999), weight_decay=0.0)  # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã‚’åˆæœŸåŒ–
        elif name == "RMSProp":  # RMSPropã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã®å ´åˆ
            optimizer = optim.RMSprop(g[2], lr=lr, momentum=momentum)  # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã‚’åˆæœŸåŒ–
        elif name == "SGD":  # SGDã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã®å ´åˆ
            optimizer = optim.SGD(g[2], lr=lr, momentum=momentum, nesterov=True)  # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã‚’åˆæœŸåŒ–
        else:  # ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã®å ´åˆ
            raise NotImplementedError(
                f"Optimizer '{name}' not found in list of available optimizers "
                f"[Adam, AdamW, NAdam, RAdam, RMSProp, SGD, auto]."
                "To request support for addition optimizers please visit https://github.com/ultralytics/ultralytics."
            )  # ã‚¨ãƒ©ãƒ¼ã‚’ç™ºç”Ÿ

        optimizer.add_param_group({"params": g[0], "weight_decay": decay})  # add g0 with weight_decayã€‚é‡ã¿æ¸›è¡°ã§g0ã‚’è¿½åŠ 
        optimizer.add_param_group({"params": g[1], "weight_decay": 0.0})  # add g1 (BatchNorm2d weights)ã€‚BatchNorm2dé‡ã¿ã‚’è¿½åŠ 
        LOGGER.info(
            f"{colorstr('optimizer:')} {type(optimizer).__name__}(lr={lr}, momentum={momentum}) with parameter groups "
            f'{len(g[1])} weight(decay=0.0), {len(g[0])} weight(decay={decay}), {len(g[2])} bias(decay=0.0)'
        )  # ãƒ­ã‚°ã‚’å‡ºåŠ›
        return optimizer  # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã‚’è¿”ã™