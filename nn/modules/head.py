# Ultralytics YOLO ğŸš€, AGPL-3.0 license
"""Model head modules."""

import copy
import math

import torch
import torch.nn as nn


from utils.tal import TORCH_1_10, dist2bbox, make_anchors

from .block import DFL
from .conv import Conv, DWConv, Conv_withoutBN


__all__ = "Detect", "v10Detect"

class Detect(nn.Module):
    """YOLO Detect head for detection models."""
    # æ¤œå‡ºãƒ¢ãƒ‡ãƒ«ç”¨ã®YOLOæ¤œå‡ºãƒ˜ãƒƒãƒ‰ã€‚

    dynamic = False  # force grid reconstruction
    export = False  # export mode
    format = None  # export format
    end2end = False  # end2end
    max_det = 300  # max_det
    shape = None
    anchors = torch.empty(0)  # init
    strides = torch.empty(0)  # init
    legacy = False  # backward compatibility for v3/v5/v8/v9 models
    xyxy = False  # xyxy or xywh output

    def __init__(self, nc=80, ch=()):
        """Initializes the YOLO detection layer with specified number of classes and channels."""
        # æŒ‡å®šã•ã‚ŒãŸã‚¯ãƒ©ã‚¹æ•°ã¨ãƒãƒ£ãƒãƒ«æ•°ã§YOLOæ¤œå‡ºãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’åˆæœŸåŒ–ã—ã¾ã™ã€‚
        super().__init__()  # è¦ªã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–
        self.nc = nc  # number of classesã€‚ã‚¯ãƒ©ã‚¹æ•°
        self.nl = len(ch)  # number of detection layersã€‚æ¤œå‡ºãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°
        self.reg_max = 16  # DFL channels (ch[0] // 16 to scale 4/8/12/16/20 for n/s/m/l/x)ã€‚DFLãƒãƒ£ãƒ³ãƒãƒ«
        self.no = nc + self.reg_max * 4  # number of outputs per anchorã€‚ã‚¢ãƒ³ã‚«ãƒ¼ã‚ãŸã‚Šã®å‡ºåŠ›æ•°
        self.stride = torch.zeros(self.nl)  # strides computed during buildã€‚ãƒ“ãƒ«ãƒ‰ä¸­ã«è¨ˆç®—ã•ã‚ŒãŸã‚¹ãƒˆãƒ©ã‚¤ãƒ‰
        c2, c3 = max((16, ch[0] // 4, self.reg_max * 4)), max(ch[0], min(self.nc, 100))  # channelsã€‚ãƒãƒ£ãƒ³ãƒãƒ«
        self.cv2 = nn.ModuleList(
            nn.Sequential(Conv(x, c2, 3), Conv(c2, c2, 3), nn.Conv2d(c2, 4 * self.reg_max, 1)) for x in ch
        )  # ç•³ã¿è¾¼ã¿ãƒ¬ã‚¤ãƒ¤ãƒ¼
        
        self.cv3 = (
            nn.ModuleList(nn.Sequential(Conv(x, c3, 3), Conv(c3, c3, 3), nn.Conv2d(c3, self.nc, 1)) for x in ch)
            if self.legacy
            else nn.ModuleList(
                nn.Sequential(
                    nn.Sequential(DWConv(x, x, 3), Conv(x, c3, 1)),
                    nn.Sequential(DWConv(c3, c3, 3), Conv(c3, c3, 1)),
                    nn.Conv2d(c3, self.nc, 1),
                )
                for x in ch
            )
        )  # ç•³ã¿è¾¼ã¿ãƒ¬ã‚¤ãƒ¤ãƒ¼
        self.dfl = DFL(self.reg_max) if self.reg_max > 1 else nn.Identity()  # DFLã‚’è¨­å®š

        if self.end2end:  # ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®å ´åˆ
            self.one2one_cv2 = copy.deepcopy(self.cv2)  # cv2ã‚’ã‚³ãƒ”ãƒ¼
            self.one2one_cv3 = copy.deepcopy(self.cv3)  # cv3ã‚’ã‚³ãƒ”ãƒ¼
        
        
    def forward(self, x):
        """Concatenates and returns predicted bounding boxes and class probabilities."""
        # äºˆæ¸¬ã•ã‚ŒãŸãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã¨ã‚¯ãƒ©ã‚¹ç¢ºç‡ã‚’é€£çµã—ã¦è¿”ã—ã¾ã™ã€‚
        if self.end2end:  # ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®å ´åˆ
            return self.forward_end2end(x)  # ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œ
        
        for i in range(self.nl):  # æ¤œå‡ºãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’åå¾©å‡¦ç†
            x[i] = torch.cat((self.cv2[i](x[i]), self.cv3[i](x[i])), 1)  # ç‰¹å¾´ã‚’é€£çµ
        if self.training:  # Training pathã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ã‚¹ã®å ´åˆ
            return x  # ç‰¹å¾´ã‚’è¿”ã™
        y = self._inference(x)  # æ¨è«–ã‚’å®Ÿè¡Œ
            
        return y if self.export else (y, x)  # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã¾ãŸã¯çµæœã‚’è¿”ã™


    def forward_end2end(self, x):
        # v10Detectãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®é †æ–¹å‘ãƒ‘ã‚¹ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
        #
        # å¼•æ•°ï¼š
        #     x (tensor): å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«ã€‚
        #
        # æˆ»ã‚Šå€¤ï¼š
        #     (dict, tensor): ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰ã§ãªã„å ´åˆã¯ã€one2manyæ¤œå‡ºã¨one2oneæ¤œå‡ºã®ä¸¡æ–¹ã®å‡ºåŠ›ã‚’å«ã‚€è¾æ›¸ã‚’è¿”ã—ã¾ã™ã€‚
        #         ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯ã€one2manyæ¤œå‡ºã¨one2oneæ¤œå‡ºã®å‡ºåŠ›ã‚’åˆ¥ã€…ã«å«ã‚€è¾æ›¸ã‚’è¿”ã—ã¾ã™ã€‚
        x_detach = [xi.detach() for xi in x]  # å…¥åŠ›ã‚’åˆ†é›¢
        one2one = [
            torch.cat((self.one2one_cv2[i](x_detach[i]), self.one2one_cv3[i](x_detach[i])), 1) for i in range(self.nl)
        ]  # one2oneã‚’è¨ˆç®—

        for i in range(self.nl):  # ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’åå¾©å‡¦ç†
            x[i] = torch.cat((self.cv2[i](x[i]), self.cv3[i](x[i])), 1)  # ç‰¹å¾´ã‚’é€£çµ
        
        if self.training:  # Training pathã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ã‚¹ã®å ´åˆ
            return {"one2many": x, "one2one": one2one}  # one2manyã¨one2oneã‚’è¿”ã™

        y = self._inference(one2one)  # æ¨è«–ã‚’å®Ÿè¡Œ
        y = self.postprocess(y.permute(0, 2, 1), self.max_det, self.nc)  # å¾Œå‡¦ç†ã‚’å®Ÿè¡Œ
        return y if self.export else (y, {"one2many": x, "one2one": one2one})  # çµæœã‚’è¿”ã™

    def _inference(self, x):
        """Decode predicted bounding boxes and class probabilities based on multiple-level feature maps."""
        # è¤‡æ•°ãƒ¬ãƒ™ãƒ«ã®ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒãƒƒãƒ—ã«åŸºã¥ã„ã¦ã€äºˆæ¸¬ã•ã‚ŒãŸãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã¨ã‚¯ãƒ©ã‚¹ç¢ºç‡ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¾ã™ã€‚
        # Inference path
        shape = x[0].shape  # BCHW
        x_cat = torch.cat([xi.view(shape[0], self.no, -1) for xi in x], 2)
        if self.format != "imx" and (self.dynamic or self.shape != shape):
            self.anchors, self.strides = (x.transpose(0, 1) for x in make_anchors(x, self.stride, 0.5))
            self.shape = shape

        if self.export and self.format in {"saved_model", "pb", "tflite", "edgetpu", "tfjs"}:  # avoid TF FlexSplitV opsã€‚TF FlexSplitV opsã‚’å›é¿
            box = x_cat[:, : self.reg_max * 4]
            cls = x_cat[:, self.reg_max * 4 :]  # åˆ†å‰²
        else:  # ãã‚Œä»¥å¤–ã®å ´åˆ
            box, cls = x_cat.split((self.reg_max * 4, self.nc), 1)  # ç‰¹å¾´ã‚’åˆ†å‰²

        if self.export and self.format in {"tflite", "edgetpu"}:  # æ¨è«–ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹å ´åˆ
            # Precompute normalization factor to increase numerical stability
            # See https://github.com/ultralytics/ultralytics/issues/7371
            grid_h = shape[2]
            grid_w = shape[3]
            grid_size = torch.tensor([grid_w, grid_h, grid_w, grid_h], device=box.device).reshape(1, 4, 1)
            norm = self.strides / (self.stride[0] * grid_size)
            dbox = self.decode_bboxes(self.dfl(box) * norm, self.anchors.unsqueeze(0) * norm[:, :2])
        elif self.export and self.format == "imx":
            dbox = self.decode_bboxes(
                self.dfl(box) * self.strides, self.anchors.unsqueeze(0) * self.strides, xywh=False
            )
            return dbox.transpose(1, 2), cls.sigmoid().permute(0, 2, 1)
        else:
            dbox = self.decode_bboxes(self.dfl(box), self.anchors.unsqueeze(0)) * self.strides

        return torch.cat((dbox, cls.sigmoid()), 1)  # ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã¨ã‚¯ãƒ©ã‚¹ã‚’é€£çµ

    def bias_init(self):
        """Initialize Detect() biases, WARNING: requires stride availability."""
        # Detect()ãƒã‚¤ã‚¢ã‚¹ã‚’åˆæœŸåŒ–ã—ã¾ã™ã€‚è­¦å‘Šï¼šã‚¹ãƒˆãƒ©ã‚¤ãƒ‰ã®å¯ç”¨æ€§ãŒå¿…è¦ã§ã™ã€‚
        m = self  # self.model[-1]  # Detect() moduleã€‚æ¤œå‡ºãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
        # cf = torch.bincount(torch.tensor(np.concatenate(dataset.labels, 0)[:, 0]).long(), minlength=nc) + 1
        # ncf = math.log(0.6 / (m.nc - 0.999999)) if cf is None else torch.log(cf / cf.sum())  # nominal class frequency
        for a, b, s in zip(m.cv2, m.cv3, m.stride):  # fromã€‚fromã‚’åå¾©å‡¦ç†
            a[-1].bias.data[:] = 1.0  # boxã€‚ãƒœãƒƒã‚¯ã‚¹
            b[-1].bias.data[: m.nc] = math.log(5 / m.nc / (640 / s) ** 2)  # cls (.01 objects, 80 classes, 640 img)ã€‚cls
        if self.end2end:  # ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®å ´åˆ
            for a, b, s in zip(m.one2one_cv2, m.one2one_cv3, m.stride):  # fromã€‚fromã‚’åå¾©å‡¦ç†
                a[-1].bias.data[:] = 1.0  # boxã€‚ãƒœãƒƒã‚¯ã‚¹
                b[-1].bias.data[: m.nc] = math.log(5 / m.nc / (640 / s) ** 2)  # cls (.01 objects, 80 classes, 640 img)ã€‚cls

    def decode_bboxes(self, bboxes, anchors):
        """Decode bounding boxes."""
        # ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¾ã™ã€‚
        return dist2bbox(bboxes, anchors, xywh=not self.end2end, dim=1)  # ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰

    @staticmethod
    def postprocess(preds: torch.Tensor, max_det: int, nc: int = 80):
        # YOLOãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã‚’å¾Œå‡¦ç†ã—ã¾ã™ã€‚
        #YOLO ã®æ¨è«–çµæœ (raw predictions) ã‚’å‡¦ç†ã—ã¦ã€æœ€ã‚‚ä¿¡é ¼æ€§ã®é«˜ã„æ¤œå‡ºçµæœã‚’å–å¾—ã™ã‚‹ãŸã‚ã®å¾Œå‡¦ç† (Post-Processing) ã‚’è¡Œã†é–¢æ•°
        # å¼•æ•°ï¼š
        #     preds (torch.Tensor): å½¢çŠ¶(batch_sizeã€num_anchorsã€4 + nc)ã®ç”Ÿã®äºˆæ¸¬ã€‚æœ€å¾Œã®æ¬¡å…ƒã®å½¢å¼ã¯[xã€yã€wã€hã€class_probs]ã§ã™ã€‚
        #     max_det (int): ç”»åƒã‚ãŸã‚Šã®æœ€å¤§æ¤œå‡ºæ•°ã€‚
        #     nc (int, optional): ã‚¯ãƒ©ã‚¹æ•°ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼š80ã€‚
        #
        # æˆ»ã‚Šå€¤ï¼š
        #     (torch.Tensor): å½¢çŠ¶(batch_sizeã€min(max_detã€num_anchors)ã€6)ã§ã€æœ€å¾Œã®æ¬¡å…ƒå½¢å¼ãŒ[xã€yã€wã€hã€max_class_probã€class_index]ã®å‡¦ç†ã•ã‚ŒãŸäºˆæ¸¬ã€‚
        batch_size, anchors, _ = preds.shape  # i.e. shape(16,8400,84)ã€‚å½¢çŠ¶ã‚’å–å¾—
        boxes, scores = preds.split([4, nc], dim=-1)  # ç‰¹å¾´ã‚’åˆ†å‰²
        index = scores.amax(dim=-1).topk(min(max_det, anchors))[1].unsqueeze(-1)  # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
        boxes = boxes.gather(dim=1, index=index.repeat(1, 1, 4))  # ãƒœãƒƒã‚¯ã‚¹ã‚’åé›†
        scores = scores.gather(dim=1, index=index.repeat(1, 1, nc))  # ã‚¹ã‚³ã‚¢ã‚’åé›†
        scores, index = scores.flatten(1).topk(min(max_det, anchors))  # ã‚¹ã‚³ã‚¢ã¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
        i = torch.arange(batch_size)[..., None]  # batch indicesã€‚ãƒãƒƒãƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        return torch.cat([boxes[i, index // nc], scores[..., None], (index % nc)[..., None].float()], dim=-1)  # çµæœã‚’è¿”ã™
    


class v10Detect(Detect):
    # https://arxiv.org/pdf/2405.14458ã‹ã‚‰ã®v10æ¤œå‡ºãƒ˜ãƒƒãƒ‰ã€‚
    #
    # å¼•æ•°ï¼š
    #     nc (int): ã‚¯ãƒ©ã‚¹æ•°ã€‚
    #     ch (tuple): ãƒãƒ£ãƒãƒ«ã‚µã‚¤ã‚ºã®ã‚¿ãƒ—ãƒ«ã€‚
    #
    # å±æ€§ï¼š
    #     max_det (int): æ¤œå‡ºã®æœ€å¤§æ•°ã€‚
    #
    # ãƒ¡ã‚½ãƒƒãƒ‰ï¼š
    #     __init__(self, nc=80, ch=()): v10Detectã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’åˆæœŸåŒ–ã—ã¾ã™ã€‚
    #     forward(self, x): v10Detectãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®é †æ–¹å‘ãƒ‘ã‚¹ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
    #     bias_init(self): Detectãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ãƒã‚¤ã‚¢ã‚¹ã‚’åˆæœŸåŒ–ã—ã¾ã™ã€‚

    end2end = True  # ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰

    def __init__(self, nc=80, ch=()):
        """Initializes the v10Detect object with the specified number of classes and input channels."""
        # æŒ‡å®šã•ã‚ŒãŸã‚¯ãƒ©ã‚¹æ•°ã¨å…¥åŠ›ãƒãƒ£ãƒãƒ«ã§v10Detectã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’åˆæœŸåŒ–ã—ã¾ã™ã€‚
        super().__init__(nc, ch)  # è¦ªã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–
        c3 = max(ch[0], min(self.nc, 100))  # channelsã€‚ãƒãƒ£ãƒ³ãƒãƒ«
        # Light cls head
        self.cv3 = nn.ModuleList(
            nn.Sequential(
                nn.Sequential(Conv(x, x, 3, g=x), Conv(x, c3, 1)),
                nn.Sequential(Conv(c3, c3, 3, g=c3), Conv(c3, c3, 1)),
                nn.Conv2d(c3, self.nc, 1),
            )
            for x in ch
        )  # ç•³ã¿è¾¼ã¿ãƒ¬ã‚¤ãƒ¤ãƒ¼
        self.one2one_cv3 = copy.deepcopy(self.cv3)  # cv3ã‚’ã‚³ãƒ”ãƒ¼
    
    def fuse(self):
        """Removes the one2many head."""
        self.cv2 = self.cv3 = nn.ModuleList([nn.Identity()] * self.nl)


class Detectv2(nn.Module):
    """
    Detectv2ã¯2ç¨®é¡ã®å…¥åŠ›å±¤ã‹ã‚‰ãã‚Œãã‚Œåˆ¥ã®æ¤œå‡ºãƒ˜ãƒƒãƒ‰ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚
    
    å…¥åŠ›:
      nc: ã‚¯ãƒ©ã‚¹æ•°
      ch: ãƒªã‚¹ãƒˆã¾ãŸã¯ã‚¿ãƒ—ãƒ«ã§2è¦ç´ 
          ch[0]: head_a ç”¨ã®å…¥åŠ›ãƒãƒ£ãƒãƒ«ãƒªã‚¹ãƒˆ (ä¾‹: [P3, P4, P5] ã®ãƒãƒ£ãƒãƒ«æ•°)
          ch[1]: head_b ç”¨ã®å…¥åŠ›ãƒãƒ£ãƒãƒ«ãƒªã‚¹ãƒˆ (ä¾‹: [P5] ã®ãƒãƒ£ãƒãƒ«æ•°)
    """
    end2end = True
    dynamic = False  # force grid reconstructionã€‚ã‚°ãƒªãƒƒãƒ‰å†æ§‹ç¯‰ã‚’å¼·åˆ¶
    export = False  # export modeã€‚ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãƒ¢ãƒ¼ãƒ‰
    max_det = 300  # max_det
    shape = None  # å½¢çŠ¶
    anchors = torch.empty(0)  # initã€‚åˆæœŸåŒ–
    strides = torch.empty(0)  # initã€‚åˆæœŸåŒ–
    legacy = False  # backward compatibility for v3/v5/v8/v9 modelsã€‚v3 / v5 / v8 / v9ãƒ¢ãƒ‡ãƒ«ã¨ã®ä¸‹ä½äº’æ›æ€§

    def __init__(self, nc=80, ch=()):
        super().__init__()
        if not isinstance(ch, list) or len(ch) != 2:
            raise ValueError("ch must be a list of [head_a_channels, head_b_channels]")
        
        self.nc = nc  # number of classesã€‚ã‚¯ãƒ©ã‚¹æ•°
        self.nl = len(ch[0])  # number of detection layersã€‚æ¤œå‡ºãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°
        self.ch=ch[1]

        self.reg_max = 16  # DFL channels (ch[0] // 16 to scale 4/8/12/16/20 for n/s/m/l/x)ã€‚DFLãƒãƒ£ãƒ³ãƒãƒ«
        self.no = nc + self.reg_max * 4  # number of outputs per anchorã€‚ã‚¢ãƒ³ã‚«ãƒ¼ã‚ãŸã‚Šã®å‡ºåŠ›æ•°
        self.stride = torch.zeros(self.nl)  # strides computed during buildã€‚ãƒ“ãƒ«ãƒ‰ä¸­ã«è¨ˆç®—ã•ã‚ŒãŸã‚¹ãƒˆãƒ©ã‚¤ãƒ‰
        c2, c3 = max((16, self.ch[0] // 4, self.reg_max * 4)), max(self.ch[0], min(self.nc, 100))  # channelsã€‚ãƒãƒ£ãƒ³ãƒãƒ«
        "ch[0]ã¨ch[1]ã®ãƒãƒ£ãƒãƒ«æ•°ã‚’ä¸€è‡´ã•ã›ã‚‹ãŸã‚ã®convå±¤"
        self.align_conv = nn.ModuleList(
           nn.Conv2d(input_ch, out_ch, kernel_size=1, stride=1)
           for input_ch, out_ch in zip(ch[0],ch[1])
        )
        self.cv2 = nn.ModuleList(
            nn.Sequential(Conv(x, c2, 3), Conv(c2, c2, 3), nn.Conv2d(c2, 4 * self.reg_max, 1)) for x in self.ch
        )  # ç•³ã¿è¾¼ã¿ãƒ¬ã‚¤ãƒ¤ãƒ¼
        
        
        self.cv3 = (
            nn.ModuleList(nn.Sequential(Conv(x, c3, 3), Conv(c3, c3, 3), nn.Conv2d(c3, self.nc, 1)) for x in self.ch)
            if self.legacy
            else nn.ModuleList(
                nn.Sequential(
                    nn.Sequential(DWConv(x, x, 3), Conv(x, c3, 1)),
                    nn.Sequential(DWConv(c3, c3, 3), Conv(c3, c3, 1)),
                    nn.Conv2d(c3, self.nc, 1),
                )
                for x in self.ch
            )
        )  # ç•³ã¿è¾¼ã¿ãƒ¬ã‚¤ãƒ¤ãƒ¼
        
        self.dfl = DFL(self.reg_max) if self.reg_max > 1 else nn.Identity()  # DFLã‚’è¨­å®š  

        self.one2one_cv2 = copy.deepcopy(self.cv2)  # cv2ã‚’ã‚³ãƒ”ãƒ¼
        self.one2one_cv3 = copy.deepcopy(self.cv3)  # cv3ã‚’ã‚³ãƒ”ãƒ¼  
        
        self.feature_maps = {
            'pre_pan': [],   # 4,6,10ã®å‡ºåŠ› (PANå‰)
            'post_pan': [],  # 23,24,25ã®å‡ºåŠ› (1x1 Convå¾Œ)
        }  
        
        self.skip_nms = False
        
        self._is_predict = False  # æ¨è«–ãƒ¢ãƒ¼ãƒ‰ãƒ•ãƒ©ã‚°ã®åˆæœŸåŒ–

        
    @property
    def is_predict(self):
        """æ¨è«–ãƒ¢ãƒ¼ãƒ‰ã‹ã©ã†ã‹ã‚’è¿”ã™"""
        return self._is_predict

    @is_predict.setter
    def is_predict(self, value):
        """æ¨è«–ãƒ¢ãƒ¼ãƒ‰ã‚’è¨­å®šã™ã‚‹"""
        self._is_predict = value

    def forward(self, x):
        """
        å…¥åŠ› x ã¯ãƒªã‚¹ãƒˆã¾ãŸã¯ã‚¿ãƒ—ãƒ«ã§ã€ä»¥ä¸‹ã®å½¢å¼ã‚’æœŸå¾…ã—ã¾ã™:
          x = [head_a_features, head_b_features]
        å„ head_x_features ã¯ã€ä¾‹: [feat1, feat2, feat3] (å„ feat ã¯ [B, C, H, W] ã®ãƒ†ãƒ³ã‚½ãƒ«)ã€‚
        è¨“ç·´æ™‚ã¯å„ãƒ˜ãƒƒãƒ‰ã®ä¸­é–“å‡ºåŠ›ã‚’è¿”ã—ã€æ¨è«–æ™‚ã¯å¾Œå‡¦ç†ã—ãŸæ¤œå‡ºçµæœã‚’è¿”ã—ã¾ã™ã€‚
        """
        if not isinstance(x, (list, tuple)) or len(x) != 2:
            raise ValueError("Input must be a list of [head_a_features, head_b_features]")
    
        pre_pan_feats, post_pan_feats = x
        
        pre_pan_feats_aligned = [
            self.align_conv[i](pre_pan_feats[i]) for i in range(self.nl)
        ]
        
        # self.feature_maps['pre_pan'] = list(pre_pan_feats_aligned)
        # self.feature_maps['post_pan'] = list(post_pan_feats)
        
        one2many = [
            torch.cat((self.cv2[i](post_pan_feats[i]), self.cv3[i](post_pan_feats[i])), 1) for i in range(self.nl)
        ]
        

        #if self.is_predict == False:
        one2one = [
                    torch.cat((self.one2one_cv2[i](pre_pan_feats_aligned[i]), self.one2one_cv3[i](pre_pan_feats_aligned[i])), 1) for i in range(self.nl)
                ]  # one2oneã‚’è¨ˆç®—
        # else:
        #     one2one = None
        
        # ç‰¹å¾´ã‚’é€£çµ
        
        if self.training:  # Training pathã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ã‚¹ã®å ´åˆ
            return {"one2many": one2many, "one2one": one2one}  # one2manyã¨one2oneã‚’è¿”ã™
        

        y = self._inference(one2many)  # æ¨è«–ã‚’å®Ÿè¡Œ
        if self.skip_nms is True:
            y = self.postprocess(y.permute(0, 2, 1), self.max_det, self.nc)  # å¾Œå‡¦ç†ã‚’å®Ÿè¡Œ
        return y if self.export else (y, {"one2many": one2many, "one2one": one2one})  # çµæœã‚’è¿”ã™

    
    
    def _inference(self, x):
        """Decode predicted bounding boxes and class probabilities based on multiple-level feature maps."""
        # è¤‡æ•°ãƒ¬ãƒ™ãƒ«ã®ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒãƒƒãƒ—ã«åŸºã¥ã„ã¦ã€äºˆæ¸¬ã•ã‚ŒãŸãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã¨ã‚¯ãƒ©ã‚¹ç¢ºç‡ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¾ã™ã€‚
        # Inference path
        shape = x[0].shape  # BCHWã€‚å½¢çŠ¶ã‚’å–å¾—
        x_cat = torch.cat([xi.view(shape[0], self.no, -1) for xi in x], 2)  # ç‰¹å¾´ã‚’é€£çµ
        if self.dynamic or self.shape != shape:  # å‹•çš„ã¾ãŸã¯å½¢çŠ¶ãŒç•°ãªã‚‹å ´åˆ
            self.anchors, self.strides = (x.transpose(0, 1) for x in make_anchors(x, self.stride, 0.5))  # ã‚¢ãƒ³ã‚«ãƒ¼ã¨ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰ã‚’ç”Ÿæˆ
            self.shape = shape  # å½¢çŠ¶ã‚’è¨­å®š

        if self.export and self.format in {"saved_model", "pb", "tflite", "edgetpu", "tfjs"}:  # avoid TF FlexSplitV opsã€‚TF FlexSplitV opsã‚’å›é¿
            box = x_cat[:, : self.reg_max * 4]
            cls = x_cat[:, self.reg_max * 4 :]  # åˆ†å‰²
        else:  # ãã‚Œä»¥å¤–ã®å ´åˆ
            box, cls = x_cat.split((self.reg_max * 4, self.nc), 1)  # ç‰¹å¾´ã‚’åˆ†å‰²

        if self.export and self.format in {"tflite", "edgetpu"}:  # æ¨è«–ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹å ´åˆ
            # Precompute normalization factor to increase numerical stability
            # See https://github.com/ultralytics/ultralytics/issues/7371
            grid_h = shape[2]  # ã‚°ãƒªãƒƒãƒ‰é«˜
            grid_w = shape[3]  # ã‚°ãƒªãƒƒãƒ‰å¹…
            grid_size = torch.tensor([grid_w, grid_h, grid_w, grid_h], device=box.device).reshape(1, 4, 1)  # ã‚°ãƒªãƒƒãƒ‰ã‚µã‚¤ã‚º
            norm = self.strides / (self.stride[0] * grid_size)  # æ­£è¦åŒ–
            dbox = self.decode_bboxes(self.dfl(box) * norm, self.anchors.unsqueeze(0) * norm[:, :2])  # ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
        else:  # ãã‚Œä»¥å¤–ã®å ´åˆ
            dbox = self.decode_bboxes(self.dfl(box), self.anchors.unsqueeze(0)) * self.strides  # ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰

        return torch.cat((dbox, cls.sigmoid()), 1)  # ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã¨ã‚¯ãƒ©ã‚¹ã‚’é€£çµ

    def bias_init(self):
        """Initialize Detect() biases, WARNING: requires stride availability."""
        # Detect()ãƒã‚¤ã‚¢ã‚¹ã‚’åˆæœŸåŒ–ã—ã¾ã™ã€‚è­¦å‘Šï¼šã‚¹ãƒˆãƒ©ã‚¤ãƒ‰ã®å¯ç”¨æ€§ãŒå¿…è¦ã§ã™ã€‚
        m = self  # self.model[-1]  # Detect() moduleã€‚æ¤œå‡ºãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
        # cf = torch.bincount(torch.tensor(np.concatenate(dataset.labels, 0)[:, 0]).long(), minlength=nc) + 1
        # ncf = math.log(0.6 / (m.nc - 0.999999)) if cf is None else torch.log(cf / cf.sum())  # nominal class frequency
        for a, b, s in zip(m.cv2, m.cv3, m.stride):  # fromã€‚fromã‚’åå¾©å‡¦ç†
            a[-1].bias.data[:] = 1.0  # boxã€‚ãƒœãƒƒã‚¯ã‚¹
            b[-1].bias.data[: m.nc] = math.log(5 / m.nc / (640 / s) ** 2)  # cls (.01 objects, 80 classes, 640 img)ã€‚cls
        for a, b, s in zip(m.one2one_cv2, m.one2one_cv3, m.stride):  # fromã€‚fromã‚’åå¾©å‡¦ç†
            a[-1].bias.data[:] = 1.0  # boxã€‚ãƒœãƒƒã‚¯ã‚¹
            b[-1].bias.data[: m.nc] = math.log(5 / m.nc / (640 / s) ** 2)  # cls (.01 objects, 80 classes, 640 img)ã€‚cls

    def decode_bboxes(self, bboxes, anchors):
        """Decode bounding boxes."""
        # ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¾ã™ã€‚
        return dist2bbox(bboxes, anchors, xywh=not self.skip_nms, dim=1)  # ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰

    @staticmethod
    def postprocess(preds: torch.Tensor, max_det: int, nc: int = 80):
        # YOLOãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã‚’å¾Œå‡¦ç†ã—ã¾ã™ã€‚
        #YOLO ã®æ¨è«–çµæœ (raw predictions) ã‚’å‡¦ç†ã—ã¦ã€æœ€ã‚‚ä¿¡é ¼æ€§ã®é«˜ã„æ¤œå‡ºçµæœã‚’å–å¾—ã™ã‚‹ãŸã‚ã®å¾Œå‡¦ç† (Post-Processing) ã‚’è¡Œã†é–¢æ•°
        # å¼•æ•°ï¼š
        #     preds (torch.Tensor): å½¢çŠ¶(batch_sizeã€num_anchorsã€4 + nc)ã®ç”Ÿã®äºˆæ¸¬ã€‚æœ€å¾Œã®æ¬¡å…ƒã®å½¢å¼ã¯[xã€yã€wã€hã€class_probs]ã§ã™ã€‚
        #     max_det (int): ç”»åƒã‚ãŸã‚Šã®æœ€å¤§æ¤œå‡ºæ•°ã€‚
        #     nc (int, optional): ã‚¯ãƒ©ã‚¹æ•°ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼š80ã€‚
        #
        # æˆ»ã‚Šå€¤ï¼š
        #     (torch.Tensor): å½¢çŠ¶(batch_sizeã€min(max_detã€num_anchors)ã€6)ã§ã€æœ€å¾Œã®æ¬¡å…ƒå½¢å¼ãŒ[xã€yã€wã€hã€max_class_probã€class_index]ã®å‡¦ç†ã•ã‚ŒãŸäºˆæ¸¬ã€‚
        batch_size, anchors, _ = preds.shape  # i.e. shape(16,8400,84)ã€‚å½¢çŠ¶ã‚’å–å¾—
        boxes, scores = preds.split([4, nc], dim=-1)  # ç‰¹å¾´ã‚’åˆ†å‰²
        index = scores.amax(dim=-1).topk(min(max_det, anchors))[1].unsqueeze(-1)  # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
        boxes = boxes.gather(dim=1, index=index.repeat(1, 1, 4))  # ãƒœãƒƒã‚¯ã‚¹ã‚’åé›†
        scores = scores.gather(dim=1, index=index.repeat(1, 1, nc))  # ã‚¹ã‚³ã‚¢ã‚’åé›†
        scores, index = scores.flatten(1).topk(min(max_det, anchors))  # ã‚¹ã‚³ã‚¢ã¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
        i = torch.arange(batch_size)[..., None]  # batch indicesã€‚ãƒãƒƒãƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        return torch.cat([boxes[i, index // nc], scores[..., None], (index % nc)[..., None].float()], dim=-1)  # çµæœã‚’è¿”ã™




