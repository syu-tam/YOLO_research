# Ultralytics YOLO ğŸš€, AGPL-3.0 license

import contextlib
import pickle
import re
import types
from copy import deepcopy
from pathlib import Path

import torch
import torch.nn as nn

from nn.modules import (
    AIFI,
    C1,
    C2,
    C2PSA,
    C3,
    C3TR,
    ELAN1,
    PSA,
    SPP,
    SPPELAN,
    SPPF,
    AConv,
    ADown,
    Bottleneck,
    BottleneckCSP,
    C2f,
    C2fAttn,
    C2fCIB,
    C2fPSA,
    C3Ghost,
    C3k2,
    C3x,
    CBFuse,
    CBLinear,
    Concat,
    Conv,
    Conv2,
    ConvTranspose,
    Detect,
    DWConv,
    DWConvTranspose2d,
    Focus,
    GhostBottleneck,
    GhostConv,
    HGBlock,
    HGStem,
    ImagePoolingAttn,
    RepC3,
    RepConv,
    RepNCSPELAN4,
    RepVGGDW,
    ResNetLayer,
    SCDown,
    v10Detect,
    Detectv2,
    Conv_withoutBN
)
from utils import DEFAULT_CFG_DICT, DEFAULT_CFG_KEYS, LOGGER, colorstr, emojis, yaml_load
from utils.checks import check_requirements, check_suffix, check_yaml
from utils.loss import (
    E2EDetectLoss,
    v8DetectionLoss,
)
from utils.ops import make_divisible
from utils.plotting import feature_visualization
from utils.torch_utils import (
    fuse_conv_and_bn,
    fuse_deconv_and_bn,
    initialize_weights,
    intersect_dicts,
    model_info,
    scale_img,
    time_sync,
)

try:
    import thop
except ImportError:
    thop = None


class BaseModel(nn.Module):
    """The BaseModel class serves as a base class for all the models in the Ultralytics YOLO family."""
    # BaseModelã‚¯ãƒ©ã‚¹ã¯ã€Ultralytics YOLOãƒ•ã‚¡ãƒŸãƒªãƒ¼ã®ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã®åŸºæœ¬ã‚¯ãƒ©ã‚¹ã¨ã—ã¦æ©Ÿèƒ½ã—ã¾ã™ã€‚

    def forward(self, x, *args, **kwargs):
        # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¾ãŸã¯æ¨è«–ã®ã„ãšã‚Œã‹ã®ãŸã‚ã«ã€ãƒ¢ãƒ‡ãƒ«ã®é †æ–¹å‘ãƒ‘ã‚¹ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
        #
        # xãŒdictã®å ´åˆã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®æå¤±ã‚’è¨ˆç®—ã—ã¦è¿”ã—ã¾ã™ã€‚ãã‚Œä»¥å¤–ã®å ´åˆã¯ã€æ¨è«–ã®äºˆæ¸¬ã‚’è¿”ã—ã¾ã™ã€‚
        #
        # å¼•æ•°ï¼š
        #     x (torch.Tensor | dict): æ¨è«–ã®å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«ã€ã¾ãŸã¯ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®ç”»åƒãƒ†ãƒ³ã‚½ãƒ«ã¨ãƒ©ãƒ™ãƒ«ã‚’å«ã‚€dictã€‚
        #     *args (Any): å¯å¤‰é•·å¼•æ•°ãƒªã‚¹ãƒˆã€‚
        #     **kwargs (Any): ä»»æ„ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¼•æ•°ã€‚
        #
        # æˆ»ã‚Šå€¤ï¼š
        #     (torch.Tensor): xãŒdictï¼ˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ï¼‰ã®å ´åˆã¯æå¤±ã€ã¾ãŸã¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯äºˆæ¸¬ï¼ˆæ¨è«–ï¼‰ã€‚
        if isinstance(x, dict):  # for cases of training and validating while trainingã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŠã‚ˆã³ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã®æ¤œè¨¼ã®å ´åˆ
            return self.loss(x, *args, **kwargs)  # æå¤±ã‚’è¨ˆç®—ã—ã¦è¿”ã™
        return self.predict(x, *args, **kwargs)  # äºˆæ¸¬ã‚’è¨ˆç®—ã—ã¦è¿”ã™

    def predict(self, x, profile=False, visualize=False, augment=False, embed=None):
        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä»‹ã—ã¦é †æ–¹å‘ãƒ‘ã‚¹ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
        #
        # å¼•æ•°ï¼š
        #     x (torch.Tensor): ãƒ¢ãƒ‡ãƒ«ã¸ã®å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«ã€‚
        #     profile (bool): Trueã®å ´åˆã€å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®è¨ˆç®—æ™‚é–“ã‚’å‡ºåŠ›ã—ã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Falseã§ã™ã€‚
        #     visualize (bool): Trueã®å ´åˆã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒãƒƒãƒ—ã‚’ä¿å­˜ã—ã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Falseã§ã™ã€‚
        #     augment (bool): äºˆæ¸¬ä¸­ã«ç”»åƒã‚’æ‹¡å¼µã—ã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Falseã§ã™ã€‚
        #     embed (list, optional): è¿”ã™ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ™ã‚¯ãƒˆãƒ«/åŸ‹ã‚è¾¼ã¿ã®ãƒªã‚¹ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰ã€‚
        #
        # æˆ»ã‚Šå€¤ï¼š
        #     (torch.Tensor): ãƒ¢ãƒ‡ãƒ«ã®æœ€å¾Œã®å‡ºåŠ›ã€‚
        if augment:  # æ‹¡å¼µã™ã‚‹å ´åˆ
            return self._predict_augment(x)  # æ‹¡å¼µã•ã‚ŒãŸäºˆæ¸¬ã‚’å®Ÿè¡Œ
        return self._predict_once(x, profile, visualize, embed)  # ä¸€åº¦ã®äºˆæ¸¬ã‚’å®Ÿè¡Œ

    def _predict_once(self, x, profile=False, visualize=False, embed=None):
        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä»‹ã—ã¦é †æ–¹å‘ãƒ‘ã‚¹ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
        #
        # å¼•æ•°ï¼š
        #     x (torch.Tensor): ãƒ¢ãƒ‡ãƒ«ã¸ã®å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«ã€‚
        #     profile (bool): Trueã®å ´åˆã€å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®è¨ˆç®—æ™‚é–“ã‚’å‡ºåŠ›ã—ã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Falseã§ã™ã€‚
        #     visualize (bool): Trueã®å ´åˆã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒãƒƒãƒ—ã‚’ä¿å­˜ã—ã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Falseã§ã™ã€‚
        #     embed (list, optional): è¿”ã™ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ™ã‚¯ãƒˆãƒ«/åŸ‹ã‚è¾¼ã¿ã®ãƒªã‚¹ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰ã€‚
        #
        # æˆ»ã‚Šå€¤ï¼š
        #     (torch.Tensor): ãƒ¢ãƒ‡ãƒ«ã®æœ€å¾Œã®å‡ºåŠ›ã€‚
        y, dt, embeddings = [], [], []  # outputsã€‚å‡ºåŠ›ã€æ™‚é–“ã€åŸ‹ã‚è¾¼ã¿
        for m in self.model:  # ãƒ¢ãƒ‡ãƒ«ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’åå¾©å‡¦ç†
            if m.f != -1:  # if not from previous layerã€‚å‰ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‹ã‚‰ã®ã‚‚ã®ã§ã¯ãªã„å ´åˆ
                x = y[m.f] if isinstance(m.f, int) else [x if j == -1 else y[j] for j in m.f]  # from earlier layersã€‚ä»¥å‰ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‹ã‚‰
            if profile:  # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã™ã‚‹å ´åˆ
                self._profile_one_layer(m, x, dt)  # ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
            x = m(x)  # runã€‚å®Ÿè¡Œ
            y.append(x if m.i in self.save else None)  # save outputã€‚å‡ºåŠ›ã‚’ä¿å­˜
            if visualize:  # å¯è¦–åŒ–ã™ã‚‹å ´åˆ
                feature_visualization(x, m.type, m.i, save_dir=visualize)  # ç‰¹å¾´ã‚’å¯è¦–åŒ–
            if embed and m.i in embed:  # åŸ‹ã‚è¾¼ã‚€å ´åˆ
                embeddings.append(nn.functional.adaptive_avg_pool2d(x, (1, 1)).squeeze(-1).squeeze(-1))  # flattenã€‚å¹³å¦åŒ–
                if m.i == max(embed):  # æœ€å¤§ã®åŸ‹ã‚è¾¼ã¿ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«é”ã—ãŸå ´åˆ
                    return torch.unbind(torch.cat(embeddings, 1), dim=0)  # å¹³å¦åŒ–ã•ã‚ŒãŸåŸ‹ã‚è¾¼ã¿ã‚’è¿”ã™
        return x  # æœ€å¾Œã®å‡ºåŠ›ã‚’è¿”ã™

    def _predict_augment(self, x):
        """Perform augmentations on input image x and return augmented inference."""
        # å…¥åŠ›ç”»åƒxã«å¯¾ã—ã¦æ‹¡å¼µã‚’å®Ÿè¡Œã—ã€æ‹¡å¼µã•ã‚ŒãŸæ¨è«–ã‚’è¿”ã—ã¾ã™ã€‚
        LOGGER.warning(
            f"WARNING âš ï¸ {self.__class__.__name__} does not support 'augment=True' prediction. "
            f"Reverting to single-scale prediction."
        )  # è­¦å‘Šãƒ­ã‚°ã‚’å‡ºåŠ›
        return self._predict_once(x)  # ä¸€åº¦ã®äºˆæ¸¬ã‚’å®Ÿè¡Œ

    def _profile_one_layer(self, m, x, dt):
        # ç‰¹å®šã®å…¥åŠ›ã«å¯¾ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®å˜ä¸€ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®è¨ˆç®—æ™‚é–“ã¨FLOPã‚’ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã—ã¾ã™ã€‚
        # çµæœã‚’æä¾›ã•ã‚ŒãŸãƒªã‚¹ãƒˆã«è¿½åŠ ã—ã¾ã™ã€‚
        #
        # å¼•æ•°ï¼š
        #     m (nn.Module): ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã™ã‚‹ãƒ¬ã‚¤ãƒ¤ãƒ¼ã€‚
        #     x (torch.Tensor): ãƒ¬ã‚¤ãƒ¤ãƒ¼ã¸ã®å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã€‚
        #     dt (list): ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®è¨ˆç®—æ™‚é–“ã‚’æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆã€‚
        #
        # æˆ»ã‚Šå€¤ï¼š
        #     None
        c = m == self.model[-1] and isinstance(x, list)  # is final layer list, copy input as inplace fixã€‚æœ€çµ‚ãƒ¬ã‚¤ãƒ¤ãƒ¼ãƒªã‚¹ãƒˆã®å ´åˆã€ã‚¤ãƒ³ãƒ—ãƒ¬ãƒ¼ã‚¹ä¿®æ­£ã¨ã—ã¦å…¥åŠ›ã‚’ã‚³ãƒ”ãƒ¼
        flops = thop.profile(m, inputs=[x.copy() if c else x], verbose=False)[0] / 1e9 * 2 if thop else 0  # GFLOPsã€‚GFLOPã‚’è¨ˆç®—
        t = time_sync()  # ç¾åœ¨æ™‚é–“ã‚’å–å¾—
        for _ in range(10):  # 10å›ç¹°ã‚Šè¿”ã™
            m(x.copy() if c else x)  # ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’å®Ÿè¡Œ
        dt.append((time_sync() - t) * 100)  # è¨ˆç®—æ™‚é–“ã‚’è¿½åŠ 
        if m == self.model[0]:  # æœ€åˆã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®å ´åˆ
            LOGGER.info(f"{'time (ms)':>10s} {'GFLOPs':>10s} {'params':>10s}  module")  # ãƒ­ã‚°ã‚’å‡ºåŠ›
        LOGGER.info(f"{dt[-1]:10.2f} {flops:10.2f} {m.np:10.0f}  {m.type}")  # ãƒ­ã‚°ã‚’å‡ºåŠ›
        if c:  # æœ€å¾Œã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®å ´åˆ
            LOGGER.info(f"{sum(dt):10.2f} {'-':>10s} {'-':>10s}  Total")  # ãƒ­ã‚°ã‚’å‡ºåŠ›

    def fuse(self, verbose=True):
        # è¨ˆç®—åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«ã€ãƒ¢ãƒ‡ãƒ«ã®`Conv2dï¼ˆï¼‰`ãƒ¬ã‚¤ãƒ¤ãƒ¼ã¨`BatchNorm2dï¼ˆï¼‰`ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’å˜ä¸€ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã«èåˆã—ã¾ã™ã€‚
        #
        # æˆ»ã‚Šå€¤ï¼š
        #     (nn.Module): èåˆã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ãŒè¿”ã•ã‚Œã¾ã™ã€‚
        if not self.is_fused():  # èåˆã•ã‚Œã¦ã„ãªã„å ´åˆ
            for m in self.model.modules():  # ãƒ¢ãƒ‡ãƒ«ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’åå¾©å‡¦ç†
                if isinstance(m, (Conv, Conv2, DWConv)) and hasattr(m, "bn"):  # Convã€Conv2ã€DWConvã§ã‚ã‚Šã€bnå±æ€§ãŒã‚ã‚‹å ´åˆ
                    if isinstance(m, Conv2):  # Conv2ã®å ´åˆ
                        m.fuse_convs()  # convã‚’èåˆ
                    m.conv = fuse_conv_and_bn(m.conv, m.bn)  # update convã€‚convã‚’æ›´æ–°
                    delattr(m, "bn")  # remove batchnormã€‚ãƒãƒƒãƒæ­£è¦åŒ–ã‚’å‰Šé™¤
                    m.forward = m.forward_fuse  # update forwardã€‚é †æ–¹å‘ã‚’æ›´æ–°
                if isinstance(m, ConvTranspose) and hasattr(m, "bn"):  # ConvTransposeã§ã‚ã‚Šã€bnå±æ€§ãŒã‚ã‚‹å ´åˆ
                    m.conv_transpose = fuse_deconv_and_bn(m.conv_transpose, m.bn)  # é€†ç•³ã¿è¾¼ã¿ã‚’èåˆ
                    delattr(m, "bn")  # remove batchnormã€‚ãƒãƒƒãƒæ­£è¦åŒ–ã‚’å‰Šé™¤
                    m.forward = m.forward_fuse  # update forwardã€‚é †æ–¹å‘ã‚’æ›´æ–°
                if isinstance(m, RepConv):  # RepConvã®å ´åˆ
                    m.fuse_convs()  # convã‚’èåˆ
                    m.forward = m.forward_fuse  # update forwardã€‚é †æ–¹å‘ã‚’æ›´æ–°
                if isinstance(m, RepVGGDW):  # RepVGGDWã®å ´åˆ
                    m.fuse()  # èåˆ
                    m.forward = m.forward_fuse  # é †æ–¹å‘ã‚’æ›´æ–°
            self.info(verbose=verbose)  # æƒ…å ±ã‚’å‡ºåŠ›

        return self  # è‡ªèº«ã‚’è¿”ã™

    def is_fused(self, thresh=10):

        # ãƒ¢ãƒ‡ãƒ«ã«ã€ç‰¹å®šã®ã—ãã„å€¤ã‚ˆã‚Šã‚‚å°‘ãªã„æ•°ã®BatchNormãƒ¬ã‚¤ãƒ¤ãƒ¼ãŒã‚ã‚‹ã‹ã©ã†ã‹ã‚’ç¢ºèªã—ã¾ã™ã€‚
        #
        # å¼•æ•°ï¼š
        #     thresh (int, optional): BatchNormãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ã—ãã„å€¤æ•°ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯10ã§ã™ã€‚
        #
        # æˆ»ã‚Šå€¤ï¼š
        #     (bool): ãƒ¢ãƒ‡ãƒ«å†…ã®BatchNormãƒ¬ã‚¤ãƒ¤ãƒ¼ã®æ•°ãŒã—ãã„å€¤ã‚ˆã‚Šå°‘ãªã„å ´åˆã¯Trueã€ãã‚Œä»¥å¤–ã®å ´åˆã¯Falseã€‚
        bn = tuple(v for k, v in nn.__dict__.items() if "Norm" in k)  # normalization layers, i.e. BatchNorm2d()ã€‚æ­£è¦åŒ–ãƒ¬ã‚¤ãƒ¤ãƒ¼
        return sum(isinstance(v, bn) for v in self.modules()) < thresh  # True if < 'thresh' BatchNorm layers in modelã€‚ãƒ¢ãƒ‡ãƒ«ã«ã€Œthreshã€BatchNormãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚ˆã‚Šå°‘ãªã„å ´åˆã€True

    def info(self, detailed=False, verbose=True, imgsz=640):
        # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’å‡ºåŠ›ã—ã¾ã™ã€‚
        #
        # å¼•æ•°ï¼š
        #     detailed (bool): Trueã®å ´åˆã€ãƒ¢ãƒ‡ãƒ«ã«é–¢ã™ã‚‹è©³ç´°æƒ…å ±ã‚’å‡ºåŠ›ã—ã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯False
        #     verbose (bool): Trueã®å ´åˆã€ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’å‡ºåŠ›ã—ã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯False
        #     imgsz (int): ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ä½¿ç”¨ã•ã‚Œã‚‹ç”»åƒã®ã‚µã‚¤ã‚ºã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯640
        return model_info(self, detailed=detailed, verbose=verbose, imgsz=imgsz)  # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’å‡ºåŠ›

    def _apply(self, fn):
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¾ãŸã¯ç™»éŒ²ã•ã‚ŒãŸãƒãƒƒãƒ•ã‚¡ã§ã¯ãªã„ã€ãƒ¢ãƒ‡ãƒ«å†…ã®ã™ã¹ã¦ã®ãƒ†ãƒ³ã‚½ãƒ«ã«é–¢æ•°ã‚’é©ç”¨ã—ã¾ã™ã€‚
        #
        # å¼•æ•°ï¼š
        #     fn (function): ãƒ¢ãƒ‡ãƒ«ã«é©ç”¨ã™ã‚‹é–¢æ•°
        #
        # æˆ»ã‚Šå€¤ï¼š
        #     (BaseModel): æ›´æ–°ã•ã‚ŒãŸBaseModelã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€‚
        self = super()._apply(fn)  # è¦ªã‚¯ãƒ©ã‚¹ã®_applyã‚’å‘¼ã³å‡ºã—
        m = self.model[-1]  # Detect()ã€‚æœ€å¾Œã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’å–å¾—
        if isinstance(m, Detect) or isinstance(m, Detectv2):  # includes all Detect subclasses like Segment, Pose, OBB, WorldDetectã€‚Detectã‚µãƒ–ã‚¯ãƒ©ã‚¹ã®å ´åˆ
            m.stride = fn(m.stride)  # ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰ã‚’æ›´æ–°
            m.anchors = fn(m.anchors)  # ã‚¢ãƒ³ã‚«ãƒ¼ã‚’æ›´æ–°
            m.strides = fn(m.strides)  # ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰ã‚’æ›´æ–°
        return self  # è‡ªèº«ã‚’è¿”ã™

    def load(self, weights, verbose=True):
        # ãƒ¢ãƒ‡ãƒ«ã«é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚
        #
        # å¼•æ•°ï¼š
        #     weights (dict | torch.nn.Module): ãƒ­ãƒ¼ãƒ‰ã™ã‚‹äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ã®é‡ã¿ã€‚
        #     verbose (bool, optional): è»¢é€ã®é€²è¡ŒçŠ¶æ³ã‚’è¨˜éŒ²ã™ã‚‹ã‹ã©ã†ã‹ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Trueã§ã™ã€‚
        model = weights["model"] if isinstance(weights, dict) else weights  # torchvision models are not dictsã€‚torchvisionãƒ¢ãƒ‡ãƒ«ã¯dictã§ã¯ã‚ã‚Šã¾ã›ã‚“
        csd = model.float().state_dict()  # checkpoint state_dict as FP32ã€‚FP32ã¨ã—ã¦ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆstate_dict
        csd = intersect_dicts(csd, self.state_dict())  # intersectã€‚äº¤å·®
        self.load_state_dict(csd, strict=False)  # loadã€‚ãƒ­ãƒ¼ãƒ‰
        if verbose:  # è©³ç´°ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆ
            LOGGER.info(f"Transferred {len(csd)}/{len(self.model.state_dict())} items from pretrained weights")  # ãƒ­ã‚°ã‚’å‡ºåŠ›
        
    
    def loss(self, batch, preds=None):
        # æå¤±ã‚’è¨ˆç®—ã—ã¾ã™ã€‚
        #
        # å¼•æ•°ï¼š
        #     batch (dict): æå¤±ã‚’è¨ˆç®—ã™ã‚‹ãƒãƒƒãƒ
        #     preds (torch.Tensor | List[torch.Tensor]): äºˆæ¸¬ã€‚
        if getattr(self, "criterion", None) is None:  # åŸºæº–ãŒãªã„å ´åˆ
            self.criterion = self.init_criterion()  # åŸºæº–ã‚’åˆæœŸåŒ–
        preds = self.forward(batch["img"]) if preds is None else preds  # äºˆæ¸¬ã‚’è¨ˆç®—
        return self.criterion(preds, batch)  # æå¤±ã‚’è¨ˆç®—

    def init_criterion(self):
        """Initialize the loss criterion for the BaseModel."""
        # BaseModelã®æå¤±åŸºæº–ã‚’åˆæœŸåŒ–ã—ã¾ã™ã€‚
        raise NotImplementedError("compute_loss() needs to be implemented by task heads")  # ã‚¨ãƒ©ãƒ¼ã‚’ç™ºç”Ÿ


class DetectionModel(BaseModel):
    
    def __init__(self, cfg="yolov8n.yaml", ch=3, nc=None, verbose=True):
        super().__init__()
        self.feature_maps = {
            'indices': {
                'pre_pan': [4, 6, 10],  # P3, P4, P5ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
                'post_pan': [16, 19, 22]  # 1x1 Convå±¤ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
            }
        }
    
        
        self.yaml = cfg if isinstance(cfg, dict) else yaml_model_load(cfg)  # cfg dictã€‚cfg dict
        if self.yaml["backbone"][0][2] == "Silence":  # ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ãŒã‚µã‚¤ãƒ¬ãƒ³ã‚¹ã®å ´åˆ
            LOGGER.warning(
                "WARNING âš ï¸ YOLOv9 `Silence` module is deprecated in favor of nn.Identity. "
                "Please delete local *.pt file and re-download the latest model checkpoint."
            )  # è­¦å‘Šãƒ­ã‚°ã‚’å‡ºåŠ›
            self.yaml["backbone"][0][2] = "nn.Identity"  # ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã‚’Identityã«å¤‰æ›´

        # Define model
        ch = self.yaml["ch"] = self.yaml.get("ch", ch)  # input channelsã€‚å…¥åŠ›ãƒãƒ£ãƒ³ãƒãƒ«
        if nc and nc != self.yaml["nc"]:  # ã‚¯ãƒ©ã‚¹æ•°ãŒç•°ãªã‚‹å ´åˆ
            LOGGER.info(f"Overriding model.yaml nc={self.yaml['nc']} with nc={nc}")  # ãƒ­ã‚°ã‚’å‡ºåŠ›
            self.yaml["nc"] = nc  # override YAML valueã€‚YAMLå€¤ã‚’ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰
        self.model, self.save = parse_model(deepcopy(self.yaml), ch=ch, verbose=verbose)  # model, savelistã€‚ãƒ¢ãƒ‡ãƒ«ã‚’è§£æ

        self.names = {i: f"{i}" for i in range(self.yaml["nc"])}  # default names dictã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®åå‰è¾æ›¸
        self.inplace = self.yaml.get("inplace", True)  # inplaceã‚’è¨­å®š
        self.end2end = getattr(self.model[-1], "end2end", False)  # end2endã‚’è¨­å®š

        # Build strides
        m = self.model[-1]  # Detect() ãƒ˜ãƒƒãƒ‰ã®å®šç¾©
        if isinstance(m, Detect) or isinstance(m, Detectv2):  # includes all Detect subclasses like Segment, Pose, OBB, WorldDetectã€‚ã™ã¹ã¦ã®Detectã‚µãƒ–ã‚¯ãƒ©ã‚¹ã‚’å«ã‚€
            s = 256  # 2x min strideã€‚2å€ã®æœ€å°ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰
            m.inplace = self.inplace  # inplaceã‚’è¨­å®š
            #ãƒ€ãƒŸãƒ¼ã®ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ã€€
            def _forward(x):
                """Performs a forward pass through the model, handling different Detect subclass types accordingly."""
                # ãƒ¢ãƒ‡ãƒ«ã‚’ä»‹ã—ã¦é †æ–¹å‘ãƒ‘ã‚¹ã‚’å®Ÿè¡Œã—ã€ãã‚Œã«å¿œã˜ã¦ç•°ãªã‚‹Detectã‚µãƒ–ã‚¯ãƒ©ã‚¹ã‚¿ã‚¤ãƒ—ã‚’å‡¦ç†ã—ã¾ã™ã€‚
                if self.end2end:  # end2endã®å ´åˆ
                    return self.forward(x)["one2many"]  # one2manyã‚’è¿”ã™
                return self.forward(x)  # é †æ–¹å‘ãƒ‘ã‚¹ã‚’è¿”ã™

            m.stride = torch.tensor([s / x.shape[-2] for x in _forward(torch.zeros(1, ch, s, s))])  # forwardã€‚é †æ–¹å‘ãƒ‘ã‚¹ã‚’å®Ÿè¡Œ
            self.stride = m.stride  # ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰ã‚’è¨­å®š
            m.bias_init()  # only run onceã€‚ä¸€åº¦ã ã‘å®Ÿè¡Œ
        else:  # ãã‚Œä»¥å¤–ã®å ´åˆ
            self.stride = torch.Tensor([32])  # default stride for i.e. RTDETRã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¹ãƒˆãƒ©ã‚¤ãƒ‰

        # Init weights, biases
        initialize_weights(self)  # é‡ã¿ã‚’åˆæœŸåŒ–
        if verbose:  # è©³ç´°ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆ
            self.info()  # æƒ…å ±ã‚’è¡¨ç¤º
            LOGGER.info("")  # ç©ºç™½è¡Œã‚’å‡ºåŠ›
            

    def _predict_once(self, x, profile=False, visualize=False, embed=None):
        """Forward pass through model."""
        y, dt = [], []

        for i, m in enumerate(self.model):
            if isinstance(m, Detectv2):
                head_pre_pan_feats = [y[idx] for idx in self.feature_maps['indices']['pre_pan']]
                head_post_pan_feats = [y[idx] for idx in self.feature_maps['indices']['post_pan']]
                x = [head_pre_pan_feats, head_post_pan_feats]
                            

            elif m.f != -1:
                x = y[m.f] if isinstance(m.f, int) else [x if j == -1 else y[j] for j in m.f]

            # é †ä¼æ’­
            x = m(x)
            
            y.append(x if m.i in (self.save or self.feature_maps['indices']['pre_pan'] or self.feature_maps['indices']['post_pan']) else None)
            
        return x
    
    def _predict_augment(self, x):
        """Perform augmentations on input image x and return augmented inference and train outputs."""
        # å…¥åŠ›ç”»åƒxã§æ‹¡å¼µã‚’å®Ÿè¡Œã—ã€æ‹¡å¼µã•ã‚ŒãŸæ¨è«–ã¨ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å‡ºåŠ›ã‚’è¿”ã—ã¾ã™ã€‚
        if getattr(self, "end2end", False) or self.__class__.__name__ != "DetectionModel":  # end2endã§ã¯ãªã„ã€ã¾ãŸã¯ã‚¯ãƒ©ã‚¹åãŒDetectionModelã§ã¯ãªã„å ´åˆ
            LOGGER.warning("WARNING âš ï¸ Model does not support 'augment=True', reverting to single-scale prediction.")  # è­¦å‘Šãƒ­ã‚°ã‚’å‡ºåŠ›
            return self._predict_once(x)  # ä¸€åº¦ã®äºˆæ¸¬ã‚’å®Ÿè¡Œ
        img_size = x.shape[-2:]  # height, widthã€‚é«˜ã•ã¨å¹…
        s = [1, 0.83, 0.67]  # scalesã€‚ã‚¹ã‚±ãƒ¼ãƒ«
        f = [None, 3, None]  # flips (2-ud, 3-lr)ã€‚ãƒ•ãƒªãƒƒãƒ—
        y = []  # outputsã€‚å‡ºåŠ›
        for si, fi in zip(s, f):  # ã‚¹ã‚±ãƒ¼ãƒ«ã¨ãƒ•ãƒªãƒƒãƒ—ã‚’åå¾©å‡¦ç†
            xi = scale_img(x.flip(fi) if fi else x, si, gs=int(self.stride.max()))  # ç”»åƒã‚’ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
            yi = super().predict(xi)[0]  # forwardã€‚é †æ–¹å‘ãƒ‘ã‚¹ã‚’å®Ÿè¡Œ
            yi = self._descale_pred(yi, fi, si, img_size)  # ãƒ‡ã‚¹ã‚±ãƒ¼ãƒ«
            y.append(yi)  # çµæœã‚’è¿½åŠ 
        y = self._clip_augmented(y)  # clip augmented tailsã€‚æ‹¡å¼µã•ã‚ŒãŸãƒ†ãƒ¼ãƒ«ã‚’ã‚¯ãƒªãƒƒãƒ—
        return torch.cat(y, -1), None  # augmented inference, trainã€‚æ‹¡å¼µã•ã‚ŒãŸæ¨è«–ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°

    @staticmethod
    def _descale_pred(p, flips, scale, img_size, dim=1):
        """De-scale predictions following augmented inference (inverse operation)."""
        # æ‹¡å¼µã•ã‚ŒãŸæ¨è«–ï¼ˆé€†æ¼”ç®—ï¼‰ã«ç¶šã„ã¦äºˆæ¸¬ã‚’ãƒ‡ã‚¹ã‚±ãƒ¼ãƒ«ã—ã¾ã™ã€‚
        p[:, :4] /= scale  # de-scaleã€‚ãƒ‡ã‚¹ã‚±ãƒ¼ãƒ«
        x, y, wh, cls = p.split((1, 1, 2, p.shape[dim] - 4), dim)  # åˆ†å‰²
        if flips == 2:  # udãƒ•ãƒªãƒƒãƒ—ã®å ´åˆ
            y = img_size[0] - y  # de-flip udã€‚udãƒ•ãƒªãƒƒãƒ—ã‚’è§£é™¤
        elif flips == 3:  # lrãƒ•ãƒªãƒƒãƒ—ã®å ´åˆ
            x = img_size[1] - x  # de-flip lrã€‚lrãƒ•ãƒªãƒƒãƒ—ã‚’è§£é™¤
        return torch.cat((x, y, wh, cls), dim)  # é€£çµ

    def _clip_augmented(self, y):
        """Clip YOLO augmented inference tails."""
        # YOLOæ‹¡å¼µæ¨è«–ãƒ†ãƒ¼ãƒ«ã‚’ã‚¯ãƒªãƒƒãƒ—ã—ã¾ã™ã€‚
        nl = self.model[-1].nl  # number of detection layers (P3-P5)ã€‚æ¤œå‡ºãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°
        g = sum(4**x for x in range(nl))  # grid pointsã€‚ã‚°ãƒªãƒƒãƒ‰ãƒã‚¤ãƒ³ãƒˆ
        e = 1  # exclude layer countã€‚é™¤å¤–ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°
        i = (y[0].shape[-1] // g) * sum(4**x for x in range(e))  # indicesã€‚ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        y[0] = y[0][..., :-i]  # largeã€‚å¤§
        i = (y[-1].shape[-1] // g) * sum(4 ** (nl - 1 - x) for x in range(e))  # indicesã€‚ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        y[-1] = y[-1][..., i:]  # smallã€‚å°
        return y  # çµæœã‚’è¿”ã™


    def init_criterion(self):
        """Initialize the loss criterion for the DetectionModel."""
        # DetectionModelã®æå¤±åŸºæº–ã‚’åˆæœŸåŒ–ã—ã¾ã™ã€‚
        #return E2EDetectLoss(self)
    
        if getattr(self, "end2end", False):  # end2endã®å ´åˆ
            return E2EDetectLoss(self)  # One2One Matching (E2E YOLO)ã€‚One2Oneãƒãƒƒãƒãƒ³ã‚°ï¼ˆE2E YOLOï¼‰
        else:  # é€šå¸¸ã®å ´åˆ
            return v8DetectionLoss(self)  # é€šå¸¸ã® YOLO æå¤±é–¢æ•°
        
    @property
    def epoch(self):
        return DetectionModel._epoch

    @epoch.setter
    def epoch(self, value):
        DetectionModel._epoch = value

    @property
    def total_epochs(self):
        return DetectionModel._total_epochs

    @total_epochs.setter
    def total_epochs(self,value):
        DetectionModel._total_epochs = value



class Ensemble(nn.ModuleList):
    """Ensemble of models."""
    # ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã€‚

    def __init__(self):
        """Initialize an ensemble of models."""
        # ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚’åˆæœŸåŒ–ã—ã¾ã™ã€‚
        super().__init__()  # è¦ªã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–

    def forward(self, x, augment=False, profile=False, visualize=False):
        """Function generates the YOLO network's final layer."""
        # é–¢æ•°ã¯YOLOãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æœ€çµ‚ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
        y = [module(x, augment, profile, visualize)[0] for module in self]  # å„ãƒ¢ãƒ‡ãƒ«ã®çµæœã‚’å–å¾—
        # y = torch.stack(y).max(0)[0]  # max ensembleã€‚æœ€å¤§ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
        # y = torch.stack(y).mean(0)  # mean ensembleã€‚å¹³å‡ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
        y = torch.cat(y, 2)  # nms ensemble, y shape(B, HW, C)ã€‚nmsã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
        return y, None  # inference, train outputã€‚æ¨è«–ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å‡ºåŠ›

# Functions ------------------------------------------------------------------------------------------------------------


@contextlib.contextmanager
def temporary_modules(modules=None, attributes=None):
    """
    Context manager for temporarily adding or modifying modules in Python's module cache (`sys.modules`).

    This function can be used to change the module paths during runtime. It's useful when refactoring code,
    where you've moved a module from one location to another, but you still want to support the old import
    paths for backwards compatibility.

    Args:
        modules (dict, optional): A dictionary mapping old module paths to new module paths.
        attributes (dict, optional): A dictionary mapping old module attributes to new module attributes.

    Example:
        ```python
        with temporary_modules({"old.module": "new.module"}, {"old.module.attribute": "new.module.attribute"}):
            import old.module  # this will now import new.module
            from old.module import attribute  # this will now import new.module.attribute
        ```

    Note:
        The changes are only in effect inside the context manager and are undone once the context manager exits.
        Be aware that directly manipulating `sys.modules` can lead to unpredictable results, especially in larger
        applications or libraries. Use this function with caution.
    """
    if modules is None:
        modules = {}
    if attributes is None:
        attributes = {}
    import sys
    from importlib import import_module

    try:
        # Set attributes in sys.modules under their old name
        for old, new in attributes.items():
            old_module, old_attr = old.rsplit(".", 1)
            new_module, new_attr = new.rsplit(".", 1)
            setattr(import_module(old_module), old_attr, getattr(import_module(new_module), new_attr))

        # Set modules in sys.modules under their old name
        for old, new in modules.items():
            sys.modules[old] = import_module(new)

        yield
    finally:
        # Remove the temporary module paths
        for old in modules:
            if old in sys.modules:
                del sys.modules[old]


class SafeClass:
    """A placeholder class to replace unknown classes during unpickling."""

    def __init__(self, *args, **kwargs):
        """Initialize SafeClass instance, ignoring all arguments."""
        pass

    def __call__(self, *args, **kwargs):
        """Run SafeClass instance, ignoring all arguments."""
        pass


class SafeUnpickler(pickle.Unpickler):
    """Custom Unpickler that replaces unknown classes with SafeClass."""

    def find_class(self, module, name):
        """Attempt to find a class, returning SafeClass if not among safe modules."""
        safe_modules = (
            "torch",
            "collections",
            "collections.abc",
            "builtins",
            "math",
            "numpy",
            # Add other modules considered safe
        )
        if module in safe_modules:
            return super().find_class(module, name)
        else:
            return SafeClass


def torch_safe_load(weight, safe_only=False):
    """
    Attempts to load a PyTorch model with the torch.load() function. If a ModuleNotFoundError is raised, it catches the
    error, logs a warning message, and attempts to install the missing module via the check_requirements() function.
    After installation, the function again attempts to load the model using torch.load().

    Args:
        weight (str): The file path of the PyTorch model.
        safe_only (bool): If True, replace unknown classes with SafeClass during loading.

    Example:
    ```python
    from nn.tasks import torch_safe_load

    ckpt, file = torch_safe_load("path/to/best.pt", safe_only=True)
    ```

    Returns:
        ckpt (dict): The loaded model checkpoint.
        file (str): The loaded filename
    """
    from utils.downloads import attempt_download_asset

    check_suffix(file=weight, suffix=".pt")
    file = attempt_download_asset(weight)  # search online if missing locally
    try:
        with temporary_modules(
            modules={
                "ultralytics.yolo.utils": "ultralytics.utils",
                "ultralytics.yolo.v8": "ultralytics.models.yolo",
                "ultralytics.yolo.data": "ultralytics.data",
            },
            attributes={
                "ultralytics.nn.modules.block.Silence": "torch.nn.Identity",  # YOLOv9e
                "ultralytics.nn.tasks.YOLOv10DetectionModel": "ultralytics.nn.tasks.DetectionModel",  # YOLOv10
                "ultralytics.utils.loss.v10DetectLoss": "ultralytics.utils.loss.E2EDetectLoss",  # YOLOv10
            },
        ):
            if safe_only:
                # Load via custom pickle module
                safe_pickle = types.ModuleType("safe_pickle")
                safe_pickle.Unpickler = SafeUnpickler
                safe_pickle.load = lambda file_obj: SafeUnpickler(file_obj).load()
                with open(file, "rb") as f:
                    ckpt = torch.load(f, pickle_module=safe_pickle)
            else:
                ckpt = torch.load(file, map_location="cpu")

    except ModuleNotFoundError as e:  # e.name is missing module name
        if e.name == "models":
            raise TypeError(
                emojis(
                    f"ERROR âŒï¸ {weight} appears to be an Ultralytics YOLOv5 model originally trained "
                    f"with https://github.com/ultralytics/yolov5.\nThis model is NOT forwards compatible with "
                    f"YOLOv8 at https://github.com/ultralytics/ultralytics."
                    f"\nRecommend fixes are to train a new model using the latest 'ultralytics' package or to "
                    f"run a command with an official Ultralytics model, i.e. 'yolo predict model=yolov8n.pt'"
                )
            ) from e
        LOGGER.warning(
            f"WARNING âš ï¸ {weight} appears to require '{e.name}', which is not in Ultralytics requirements."
            f"\nAutoInstall will run now for '{e.name}' but this feature will be removed in the future."
            f"\nRecommend fixes are to train a new model using the latest 'ultralytics' package or to "
            f"run a command with an official Ultralytics model, i.e. 'yolo predict model=yolov8n.pt'"
        )
        check_requirements(e.name)  # install missing module
        ckpt = torch.load(file, map_location="cpu")

    if not isinstance(ckpt, dict):
        # File is likely a YOLO instance saved with i.e. torch.save(model, "saved_model.pt")
        LOGGER.warning(
            f"WARNING âš ï¸ The file '{weight}' appears to be improperly saved or formatted. "
            f"For optimal results, use model.save('filename.pt') to correctly save YOLO models."
        )
        ckpt = {"model": ckpt.model}

    return ckpt, file


def attempt_load_weights(weights, device=None, inplace=True, fuse=False):
    """Loads an ensemble of models weights=[a,b,c] or a single model weights=[a] or weights=a."""
    ensemble = Ensemble()
    for w in weights if isinstance(weights, list) else [weights]:
        ckpt, w = torch_safe_load(w)  # load ckpt
        args = {**DEFAULT_CFG_DICT, **ckpt["train_args"]} if "train_args" in ckpt else None  # combined args
        model = (ckpt.get("ema") or ckpt["model"]).to(device).float()  # FP32 model

        # Model compatibility updates
        model.args = args  # attach args to model
        model.pt_path = w  # attach *.pt file path to model
        model.task = guess_model_task(model)
        if not hasattr(model, "stride"):
            model.stride = torch.tensor([32.0])

        # Append
        ensemble.append(model.fuse().eval() if fuse and hasattr(model, "fuse") else model.eval())  # model in eval mode

    # Module updates
    for m in ensemble.modules():
        if hasattr(m, "inplace"):
            m.inplace = inplace
        elif isinstance(m, nn.Upsample) and not hasattr(m, "recompute_scale_factor"):
            m.recompute_scale_factor = None  # torch 1.11.0 compatibility

    # Return model
    if len(ensemble) == 1:
        return ensemble[-1]

    # Return ensemble
    LOGGER.info(f"Ensemble created with {weights}\n")
    for k in "names", "nc", "yaml":
        setattr(ensemble, k, getattr(ensemble[0], k))
    ensemble.stride = ensemble[int(torch.argmax(torch.tensor([m.stride.max() for m in ensemble])))].stride
    assert all(ensemble[0].nc == m.nc for m in ensemble), f"Models differ in class counts {[m.nc for m in ensemble]}"
    return ensemble


def attempt_load_one_weight(weight, device=None, inplace=True, fuse=False):
    """Loads a single model weights."""
    ckpt, weight = torch_safe_load(weight)  # load ckpt
    args = {**DEFAULT_CFG_DICT, **(ckpt.get("train_args", {}))}  # combine model and default args, preferring model args
    model = (ckpt.get("ema") or ckpt["model"]).to(device).float()  # FP32 model

    # Model compatibility updates
    model.args = {k: v for k, v in args.items() if k in DEFAULT_CFG_KEYS}  # attach args to model
    model.pt_path = weight  # attach *.pt file path to model
    model.task = guess_model_task(model)
    if not hasattr(model, "stride"):
        model.stride = torch.tensor([32.0])

    model = model.fuse().eval() if fuse and hasattr(model, "fuse") else model.eval()  # model in eval mode

    # Module updates
    for m in model.modules():
        if hasattr(m, "inplace"):
            m.inplace = inplace
        elif isinstance(m, nn.Upsample) and not hasattr(m, "recompute_scale_factor"):
            m.recompute_scale_factor = None  # torch 1.11.0 compatibility

    # Return model and ckpt
    return model, ckpt


def parse_model(d, ch, verbose=True):  # model_dict, input_channels(3)
    """YOLOã®model.yamlã‚’PyTorchãƒ¢ãƒ‡ãƒ«ã«å¤‰æ›ã™ã‚‹é–¢æ•°"""
    import ast

    # äº’æ›æ€§ã®ãŸã‚ã®ãƒ•ãƒ©ã‚°ï¼ˆv3/v5/v8/v9 ã®ãƒ¢ãƒ‡ãƒ«ã‚’ã‚µãƒãƒ¼ãƒˆï¼‰
    legacy = True  # å¾Œæ–¹äº’æ›æ€§ç¢ºä¿
    max_channels = float("inf")  # æœ€å¤§ãƒãƒ£ãƒ³ãƒãƒ«æ•°ã‚’ç„¡é™å¤§ã«è¨­å®š
    
    # ãƒ¢ãƒ‡ãƒ«è¨­å®šã®å–å¾—ï¼ˆå­˜åœ¨ã—ãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼‰
    nc, act, scales = (d.get(x) for x in ("nc", "activation", "scales"))  # ã‚¯ãƒ©ã‚¹æ•°ã€æ´»æ€§åŒ–é–¢æ•°ã€ã‚¹ã‚±ãƒ¼ãƒ«æƒ…å ±
    depth, width, kpt_shape = (d.get(x, 1.0) for x in ("depth_multiple", "width_multiple", "kpt_shape"))  # ã‚¹ã‚±ãƒ¼ãƒ«ä¿‚æ•°
    
    if scales:  # ã‚¹ã‚±ãƒ¼ãƒ«æƒ…å ±ãŒå­˜åœ¨ã™ã‚‹å ´åˆ
        scale = d.get("scale")  # ã‚¹ã‚±ãƒ¼ãƒ«æƒ…å ±ã‚’å–å¾—
        if not scale:
            scale = tuple(scales.keys())[0]  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚¹ã‚±ãƒ¼ãƒ«ã‚’å–å¾—
            LOGGER.warning(f"WARNING âš ï¸ no model scale passed. Assuming scale='{scale}'.")
        depth, width, max_channels = scales[scale]  # ã‚¹ã‚±ãƒ¼ãƒ«ã«å¿œã˜ã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´

    if act:  # æ´»æ€§åŒ–é–¢æ•°ã®è¨­å®š
        Conv.default_act = eval(act)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®æ´»æ€§åŒ–é–¢æ•°ã‚’æ›´æ–°
        if verbose:
            LOGGER.info(f"{colorstr('activation:')} {act}")  # è¨­å®šã‚’ãƒ­ã‚°å‡ºåŠ›

    if verbose:  # è©³ç´°æƒ…å ±ã®è¡¨ç¤º
        LOGGER.info(f"\n{'':>3}{'from':>20}{'n':>3}{'params':>10}  {'module':<45}{'arguments':<30}")
    
    ch = [ch]  # å…¥åŠ›ãƒãƒ£ãƒ³ãƒãƒ«ãƒªã‚¹ãƒˆ
    layers, save, c2 = [], [], ch[-1]  # å„å±¤ã®ãƒªã‚¹ãƒˆã€ä¿å­˜ãƒªã‚¹ãƒˆã€å‡ºåŠ›ãƒãƒ£ãƒ³ãƒãƒ«
    
    # ãƒ¢ãƒ‡ãƒ«ã®ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã¨ãƒ˜ãƒƒãƒ‰éƒ¨åˆ†ã‚’æ§‹ç¯‰
    for i, (f, n, m, args) in enumerate(d["backbone"] + d["head"]):  # from, number, module, args
        m = getattr(torch.nn, m[3:]) if "nn." in m else globals()[m]  # ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å–å¾—
        
        # å¼•æ•°ã®æ–‡å­—åˆ—ã‚’é©åˆ‡ãªå‹ã«å¤‰æ›
        for j, a in enumerate(args):
            if isinstance(a, str):
                try:
                    args[j] = locals()[a] if a in locals() else ast.literal_eval(a)  # æ–‡å­—åˆ—ã‹ã‚‰é©åˆ‡ãªå‹ã«å¤‰æ›
                except ValueError:
                    pass
        
        n = n_ = max(round(n * depth), 1) if n > 1 else n  # depthå€ç‡é©ç”¨
        
        # å„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã”ã¨ã®ãƒãƒ£ãƒ³ãƒãƒ«æ•°è¨­å®š
        if m in {Conv, ConvTranspose, GhostConv, Bottleneck, GhostBottleneck, SPP, SPPF, C2fPSA,
                 C2PSA, DWConv, Focus, BottleneckCSP, C1, C2, C2f, C3k2, RepNCSPELAN4, ELAN1, ADown, AConv, SPPELAN,
                 C2fAttn, C3, C3TR, C3Ghost, nn.ConvTranspose2d, DWConvTranspose2d, C3x, RepC3, PSA, SCDown,
                 C2fCIB, Conv_withoutBN}:
            c1, c2 = ch[f], args[0]  # å…¥å‡ºåŠ›ãƒãƒ£ãƒ³ãƒãƒ«å–å¾—
            if c2 != nc:  # ã‚¯ãƒ©ã‚¹æ•°ã¨ç•°ãªã‚‹å ´åˆã€é©åˆ‡ãªãƒãƒ£ãƒ³ãƒãƒ«æ•°ã«èª¿æ•´
                c2 = make_divisible(min(c2, max_channels) * width, 8)
            if m is C2fAttn:  # ç‰¹å®šãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®è¨­å®š
                args[1] = make_divisible(min(args[1], max_channels // 2) * width, 8)  # åŸ‹ã‚è¾¼ã¿ãƒãƒ£ãƒãƒ«æ•°
                args[2] = int(max(round(min(args[2], max_channels // 2 // 32)) * width, 1) if args[2] > 1 else args[2])
            args = [c1, c2, *args[1:]]  # å¼•æ•°æ›´æ–°
            if m in {BottleneckCSP, C1, C2, C2f, C3k2, C2fAttn, C3, C3TR, C3Ghost, C3x, RepC3, C2fPSA, C2fCIB, C2PSA}:
                args.insert(2, n)  # ç¹°ã‚Šè¿”ã—æ•°ã‚’è¿½åŠ 
                n = 1  # nã‚’1ã«ãƒªã‚»ãƒƒãƒˆ
            if m in {C3k2}:
                legacy=False
                if scale in "mlx":
                    args[3] = True
             
        elif m is AIFI:
            args = [ch[f], *args]
        elif m in {HGStem, HGBlock}:
            c1, cm, c2 = ch[f], args[0], args[1]
            args = [c1, cm, c2, *args[2:]]
            if m is HGBlock:
                args.insert(4, n)
                n = 1
        elif m is ResNetLayer:
            c2 = args[1] if args[3] else args[1] * 4
        elif m is nn.BatchNorm2d:
            args = [ch[f]]
        elif m is Concat:
            c2 = sum(ch[x] for x in f)
        elif m in {Detect, ImagePoolingAttn, v10Detect,Detectv2}:
            if isinstance(f, list) and f and isinstance(f[0], list):
                args.append([[ch[x] for x in sub_f] for sub_f in f])
            else:
                args.append([ch[x] for x in f])
            if m is Detect:
                m.legacy = legacy
        elif m is CBLinear:
            c2 = args[0]
            c1 = ch[f]
            args = [c1, c2, *args[1:]]
        elif m is CBFuse:
            c2 = ch[f[-1]]
        else:
            c2 = ch[f]
        
        # ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆï¼ˆç¹°ã‚Šè¿”ã—æ•°ã«å¿œã˜ã¦å‡¦ç†ï¼‰
        m_ = nn.Sequential(*(m(*args) for _ in range(n))) if n > 1 else m(*args)
        t = str(m)[8:-2].replace("__main__.", "")  # ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ç¨®é¡ã‚’æ–‡å­—åˆ—åŒ–
        m_.np = sum(x.numel() for x in m_.parameters())  # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°
        m_.i, m_.f, m_.type = i, f, t  # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨ã‚¿ã‚¤ãƒ—æƒ…å ±ã‚’æ ¼ç´
        if verbose:
            LOGGER.info(f"{i:>3}{str(f):>20}{n_:>3}{m_.np:10.0f}  {t:<45}{str(args):<30}")  # ãƒ­ã‚°å‡ºåŠ›
        
        if isinstance(f, list):
            f_flat = list(flatten(f))
            if i != 0:
                save.extend(x % i for x in f_flat if x != -1)
            # ã‚‚ã— i==0 ã®å ´åˆã¯ä½•ã‚‚è¿½åŠ ã—ãªã„
        else:
            if i != 0:
                save.append(f % i)
    # i==0 ã®å ´åˆã¯ä¿å­˜ã—ãªã„



        layers.append(m_)  # ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«è¿½åŠ 
        if i == 0:
            ch = []
        ch.append(c2)
    
    return nn.Sequential(*layers), sorted(save)  # æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã‚’è¿”ã™

def flatten(lst):
    """å†å¸°çš„ã«ãƒªã‚¹ãƒˆã‚’å¹³å¦åŒ–ã™ã‚‹ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿"""
    for el in lst:
        if isinstance(el, list):
            yield from flatten(el)
        else:
            yield el



def yaml_model_load(path):
    """Load a YOLOv8 model from a YAML file."""
    path = Path(path)
    if path.stem in (f"yolov{d}{x}6" for x in "nsmlx" for d in (5, 8)):
        new_stem = re.sub(r"(\d+)([nslmx])6(.+)?$", r"\1\2-p6\3", path.stem)
        LOGGER.warning(f"WARNING âš ï¸ Ultralytics YOLO P6 models now use -p6 suffix. Renaming {path.stem} to {new_stem}.")
        path = path.with_name(new_stem + path.suffix)

    # yolov11_e2e[nslmx].yaml -> yolov11_e2e.yaml ã®å¤‰æ›ã‚’è¿½åŠ 
    match = re.match(r"(yolov\d+_e2e)([nslmx])?(.+)?$", path.stem)
    if match:
        new_stem = match.group(1)
        path = path.with_name(new_stem + path.suffix)


    unified_path = re.sub(r"(yolov\d+)(?:[nslmx])?(_e2e[nslmx]?)?(.+)?$", r"\1\3", str(path))

    yaml_file = check_yaml(unified_path, hard=False) or check_yaml(path)
    d = yaml_load(yaml_file)  # model dict
    d["scale"] = guess_model_scale(path)
    d["yaml_file"] = str(path)
    return d


import re
from pathlib import Path

def guess_model_scale(model_path):
    """
    Takes a path to a YOLO model's YAML file as input and extracts the size character of the model's scale. The function
    uses regular expression matching to find the pattern of the model scale in the YAML file name, which is denoted by
    n, s, m, l, or x. The function returns the size character of the model scale as a string.

    Args:
        model_path (str | Path): The path to the YOLO model's YAML file.

    Returns:
        (str): The size character of the model's scale, which can be n, s, m, l, or x, or "" if not found.
    """
    try:
        match = re.search(r"yolo[v]?\d+([nslmx])", Path(model_path).stem)
        if match:
            return match.group(1)
        
        # yolov11_e2es ã®ã‚ˆã†ãªã‚±ãƒ¼ã‚¹ã«å¯¾å¿œ
        match = re.search(r"yolov\d+_e2e([nslmx])?", Path(model_path).stem)
        if match and match.group(1):
            return match.group(1)
            
        return "" # ã‚¹ã‚±ãƒ¼ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ç©ºæ–‡å­—ã‚’è¿”ã™
    except AttributeError:
        return ""


def guess_model_task(model):
    """
    Guess the task of a PyTorch model from its architecture or configuration.

    Args:
        model (nn.Module | dict): PyTorch model or model configuration in YAML format.

    Returns:
        (str): Task of the model ('detect', 'segment', 'classify', 'pose').

    Raises:
        SyntaxError: If the task of the model could not be determined.
    """

    def cfg2task(cfg):
        """Guess from YAML dictionary."""
        m = cfg["head"][-1][-2].lower()  # output module name
        if m in {"classify", "classifier", "cls", "fc"}:
            return "classify"
        if "detect" in m:
            return "detect"
        if m == "segment":
            return "segment"
        if m == "pose":
            return "pose"
        if m == "obb":
            return "obb"

    # Guess from model cfg
    if isinstance(model, dict):
        try:
            return cfg2task(model)
        except Exception:
            pass

    # Guess from PyTorch model
    if isinstance(model, nn.Module):  # PyTorch model
        for x in "model.args", "model.model.args", "model.model.model.args":
            try:
                return eval(x)["task"]
            except Exception:
                pass
        for x in "model.yaml", "model.model.yaml", "model.model.model.yaml":
            try:
                return cfg2task(eval(x))
            except Exception:
                pass

        for m in model.modules():
            if isinstance(m, (Detect,v10Detect)):
                return "detect"

    # Guess from model filename
    if isinstance(model, (str, Path)):
        model = Path(model)
        if "detect" in model.parts:
            return "detect"

    # Unable to determine task from model
    LOGGER.warning(
        "WARNING âš ï¸ Unable to automatically guess model task, assuming 'task=detect'. "
        "Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'."
    )
    return "detect"  # assume detect
